import pandas as pd
import numpy as np
import prophet
pd.set_option('display.max_columns', 500)
import seaborn as sns
import matplotlib.pyplot as plt
pd.set_option("mode.use_inf_as_na", True)
from matplotlib.ticker import FuncFormatter
import math
import itertools
import datetime
from datetime import date

import xgboost as xgb
from xgboost import XGBRegressor

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LinearRegression

data = pd.read_csv('sell_in_rev_v4.csv',low_memory=False,dtype={'445 Year':str
                                                                , '445 Quarter':str
                                                                , '445 Month': str
                                                                , '445 Week':str})

data['SALES_ORDER_NUMBER'] = data['SALES_ORDER_NUMBER'].fillna('0')
data['SALES_ORDER_NUMBER'] = data['SALES_ORDER_NUMBER'].replace(to_replace="NOT_DEFINED",value="0")
data['SALES_ORDER_NUMBER'] = data['SALES_ORDER_NUMBER'].replace(to_replace="NOT_DEFINED",value="0")
data['SALES_ORDER_NUMBER'] = data['SALES_ORDER_NUMBER'].astype(np.int64)
data['SALES_ORDER_NUMBER'] = data['SALES_ORDER_NUMBER'].astype('str')

data['Invoice Transaction Number'] = data['Invoice Transaction Number'].fillna('0')
data['Invoice Transaction Number'] = data['Invoice Transaction Number'].replace(to_replace="NOT_DEFINED",value="0")
data['Invoice Transaction Number'] = data['Invoice Transaction Number'].replace(to_replace="NOT_DEFINED",value="0")
data['Invoice Transaction Number'] = data['Invoice Transaction Number'].astype(np.int64)
data['Invoice Transaction Number'] = data['Invoice Transaction Number'].astype('str')

data['B2 Customer Number'] = data['B2 Customer Number'].fillna('0')
data['B2 Customer Number'] = data['B2 Customer Number'].replace(to_replace="NOT_DEFINED",value="0")
data['B2 Customer Number'] = data['B2 Customer Number'].astype('int64')
data['B2 Customer Number'] = data['B2 Customer Number'].astype('str')

data['S2 Customer Number'] = data['S2 Customer Number'].fillna('0')
data['S2 Customer Number'] = data['S2 Customer Number'].replace(to_replace="NOT_DEFINED",value="0")
data['S2 Customer Number'] = data['S2 Customer Number'].astype('int64')
data['S2 Customer Number'] = data['S2 Customer Number'].astype('str')

data['O2 Customer Number'] = data['O2 Customer Number'].fillna('0')
data['O2 Customer Number'] = data['O2 Customer Number'].replace(to_replace="NOT_DEFINED",value="0")
data['O2 Customer Number'] = data['O2 Customer Number'].astype('int64')
data['O2 Customer Number'] = data['O2 Customer Number'].astype('str')

data = data[data['MDM_COMPANY']!='NIOS']

rem = ["IO SCANNER"]
data2 = data.copy()
data2 = data2[~data2['Prod Cat L1'].isin(rem)]

df = pd.read_csv('SAP_VA05_2019.csv',low_memory=False)
df1 = pd.read_csv('SAP_VA05_2020.csv',low_memory=False)
df2 = pd.read_csv('SAP_VA05_2021.csv',low_memory=False)
df3 = pd.read_csv('SAP_VA05_2022.csv',low_memory=False)
df4 = pd.read_csv('SAP_VA05_2023.csv',low_memory=False)

va05 = pd.concat([df,df1,df2,df3,df4],ignore_index=True).drop_duplicates().reset_index()
ren = {'index': 'index_delete'
        , '  Document ': 'SALES_ORDER_NUMBER'
        , 'Req.dlv.dt': 'DELIV_DATE'
        , 'Customer': 'sap_num'
        , 'Name of customer ZB           ': 'mdm_name_va05'
        , 'Ship-To': 'sap_num_ship'
        , 'Name of the ship-to party          ':'ship_name'
        , 'PO Number': 'po_number'
        , 'DChl' : 'dist_chnl'}

va05.rename(columns=ren,inplace=True)
va05 = va05.drop(columns=['index_delete'])
va05['SALES_ORDER_NUMBER'] = va05['SALES_ORDER_NUMBER'].astype(str)
va05['sap_num'] = va05['sap_num'].astype(str)

data3 = pd.merge(data2, va05, left_on='SALES_ORDER_NUMBER',right_on="SALES_ORDER_NUMBER", how="left")

comm ={'COMMISSIONABLE':'COMM',
'NOT COMMISSIONED':'NON_COMM',
'VET ONLY':'NON_COMM',
'NOT-DEFINED' :'NON_COMM',
'DISCONTINUED':'NON_COMM'}

data3['Prod Cat Commission'] = data3['Prod Cat Commission'].map(comm)

col = ['Dp Salesordernumber','Dp Purchaseordernumber','Title', 'Ticketnumber', 
        'Customeridname','Mdm Id']
crm = pd.read_csv('c_crm_mapping.csv', low_memory=False,usecols=col)
elt = ['HEARTLAND',
'SPECIALTY DENTAL BRANDS',
'DENTAL WHALE (GPO)',
'DENTAL CARE ALLIANCE',
'42 NORTH',
'NORTH AMERICAN DENTAL GROUP',
'SMILE BRANDS',
'DENTOLOGIE',
'DENTISTRY FOR CHILDREN-D4C',
'OMS 360',
'SMILE SOURCE (GPO)',
'US ORAL SURGERY MANAGEMENT',
'DENTAL DEPOT',
'PARK DENTAL',
'ROCK DENTAL',
'INTERDENT',
'MB2',
'SONRAVA-WESTERN DENTAL',
'PACIFIC DENTAL SERVICES',
'123DENTIST',
'DENTALCORP']

pattern ='|'.join(elt)

crm['elt'] = crm['Title'].str.contains(pattern, case=False).apply(lambda x: 'ELITE DSO' if x else np.NaN)

cMap = {'Dp Purchaseordernumber':'po_number_crm','Dp Salesordernumber':'SALES_ORDER_NUMBER_crm'}
crm = crm.rename(columns=cMap)
crmSo = crm[~crm['SALES_ORDER_NUMBER_crm'].isnull()]
crmSo = crmSo.drop_duplicates(subset=['SALES_ORDER_NUMBER_crm'],keep='first')
crmPo = crm[~crm['po_number_crm'].isnull()]
crmPo = crmPo[~crmPo['Ticketnumber'].isin(list(crmSo['Ticketnumber'].drop_duplicates()))]
crmPo = crmPo.drop_duplicates(subset=['po_number_crm'],keep='first')



data4 = pd.merge(data3,crmSo,left_on='SALES_ORDER_NUMBER',right_on='SALES_ORDER_NUMBER_crm',how='left')

data4 = data4.drop(columns=['Ticketnumber', 'Title', 'po_number_crm',
       'Customeridname', 'SALES_ORDER_NUMBER_crm' ])
data5 = pd.merge(data4,crmPo,left_on='po_number',right_on='po_number_crm',how='left')
data5['elt_x'].fillna(data5['elt_y'], inplace=True)
data5['elt_x'].fillna(data5['SHIP_MDM_SDS_CUST_CLASS3'], inplace=True)
data5['SHIP_MDM_SDS_CUST_CLASS3'] = data5['elt_x'].copy()

prdMap = {
'2D_DEXIS 2D_DEXIS OP 2D PAN' :  'PRODUCT',
'2D_DEXIS 2D_DEXIS OP 3D PAN UPGRADABLE' :  'PRODUCT',
'2D_DEXIS 2D_DEXIS OP 3D PAN/CEPH UPGRADABLE' :  'PRODUCT',
'2D_DEXIS 2D_DEXIS OP 3D PRO 2D PAN' :  'PRODUCT',
'2D_DEXIS 2D_DEXIS OP 3D PRO 2D PAN/CEPH' :  'PRODUCT',
'2D_DEXIS 2D_DEXIS OP 3D PRO UPGRADE' :  'UPGRADES',
'2D_GENDEX 2D_GXDP-300 PAN' :  'PRODUCT',
'2D_GENDEX 2D_GXDP-700 PAN' :  'PRODUCT',
'2D_GENDEX 2D_GXDP-700 PAN/CEPH' :  'PRODUCT',
'2D_GENDEX 2D_GXDP-700 UPGRADE 2D' :  'UPGRADES',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OC200D PAN/CEPH' :  'PRODUCT',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP200D PAN' :  'PRODUCT',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP200D UPGRADE TO CEPH' :  'UPGRADES',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP30' :  'PRODUCT',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP300 PAN' :  'PRODUCT',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP300 PAN/CEPH' :  'PRODUCT',
'2D_SOFTWARE 2D_SOFTWARE 2D' :  'SOFTWARE',
'2D_SOREDEX 2D_CRANEX D  CEPH' :  'PRODUCT',
'2D_SOREDEX 2D_CRANEX NOVUS E' :  'PRODUCT',
'2D_SPARE PARTS_SPARE PARTS 2D' :  'PARTS',
'2D_UPGRD/CONV_DEXIS OP 3D PRO UPGRADE' :  'UPGRADES',
'2D_UPGRD/CONV_INSTRUMENTARIUM OP200D UPGRADE TO CEPH' :  'UPGRADES',
'2D_UPGRD/CONV_SOFTWARE 2D' :  'SOFTWARE',
'3D_DEXIS 3D_DEXIS OP 3D' :  'PRODUCT',
'3D_DEXIS 3D_DEXIS OP 3D LX PAN' :  'PRODUCT',
'3D_DEXIS 3D_DEXIS OP 3D PAN' :  'PRODUCT',
'3D_DEXIS 3D_DEXIS OP 3D PAN/CEPH' :  'PRODUCT',
'3D_DEXIS 3D_DEXIS OP 3D PRO' :  'PRODUCT',
'3D_DEXIS 3D_DEXIS OP 3D PRO MFOV PAN' :  'PRODUCT',
'3D_DEXIS 3D_DEXIS OP 3D PRO MFOV PAN/CEPH' :  'PRODUCT',
'3D_DEXIS 3D_DEXIS OP 3D PRO SFOV' :  'PRODUCT',
'3D_DEXIS 3D_DEXIS OP 3D PRO SFOV CEPH' :  'PRODUCT',
'3D_GENDEX 3D_GXDP-700 6X4  / 6X8 SFOV' :  'PRODUCT',
'3D_GENDEX 3D_GXDP-700 UPGRADE' :  'UPGRADES',
'3D_GENDEX 3D_GXDP-700 UPGRADE TO CEPH' :  'PRODUCT',
'3D_I-CAT 3D_HW LEGACY CONVKIT' :  'PRODUCT',
'3D_I-CAT 3D_I-CAT FLX V10' :  'PRODUCT',
'3D_I-CAT 3D_I-CAT FLX V17' :  'PRODUCT',
'3D_I-CAT 3D_I-CAT FLX V8' :  'PRODUCT',
'3D_I-CAT 3D_I-CAT NEXT-GEN LFOV' :  'PRODUCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 6X8 SFOV PAN' :  'PRODUCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 6X8 UPGRADE TO 8X15 MFOV' :  'UPGRADES',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO  PAN' :  'PRODUCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO 13X15 MFOV' :  'PRODUCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO 8X15 MFOV' :  'PRODUCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO UPGRADE 8X15 TO 13X15 LFOV' :  'PRODUCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 UPGRADE PAN TO 8X15 MFOV' :  'PRODUCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 UPGRADE TO CEPH CEPH' :  'UPGRADES',
'3D_SOREDEX 3D_CRANEX 3D' :  'PRODUCT',
'3D_SOREDEX 3D_CRANEX 3D 6X4 / 6X8 SFOV/CEPH' :  'PRODUCT',
'3D_SOREDEX 3D_CRANEX 3D UPGRADE PAN TO 3D 6X8 SFOV' :  'UPGRADES',
'3D_SPARE PARTS 3D_SPARE PARTS 3D' :  'PARTS',
'3D_SPARE PARTS I-CAT_SPARE PARTS I-CAT' :  'PARTS',
'3D_UPGRD/CONV_CRANEX 3D' :  'UPGRADES',
'3D_UPGRD/CONV_DEXIS OP 3D PRO CEPH' :  'UPGRADES',
'3D_UPGRD/CONV_DEXIS OP 3D UPGRADE' :  'UPGRADES',
'3D_UPGRD/CONV_GXDP-700 UPGRADE TO CEPH' :  'UPGRADES',
'3D_UPGRD/CONV_I-CAT FLX V17' :  'UPGRADES',
'3D_UPGRD/CONV_INSTRUMENTARIUM OP300 MAXIO UPGRADE 8X15 TO 13X15 LFOV' :  'UPGRADES',
'ACCESSORIES_ACCESSORIES_CDCM-ACCESSORIES' :  'PARTS',
'CR-READER_GENDEX CR-READER_DENOPTIX' :  'PRODUCT',
'CR-READER_GENDEX CR-READER_GXPS-500' :  'PRODUCT',
'CR-READER_INSTRUMENTARIUM CR-READER_OPTIME' :  'PRODUCT',
'CR-READER_SPARE PARTS CR-READER_SPARE PART CR-READER DENOPTIX' :  'PARTS',
'CR-READER_SPARE PARTS CR-READER_SPARE PART CR-READER GXPS-500' :  'PARTS',
'CR-READER_SPARE PARTS CR-READER_SPARE PART CR-READER OPTIME' :  'PARTS',
'CR-READER_SPARE PARTS CR-READER_SPARE PARTS CR-READER' :  'PARTS',
'INOR SOFTWARE_SOFTWARE_CLINIVIEW' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_CLINIVIEW 11' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXCLAIM' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIMAGE' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIMPLANT' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS IMAGING SUITE' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS INTEGRATOR DENTRIX' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS INTEGRATOR EZDENTAL' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS PREMIUM EAGLESOFT INTEGRATOR' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS SOFTWARE' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXNET' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXPAN' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXPSP' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXRAY' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXSCAN' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXVOICE' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXWRITE' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DX IS' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_GXTWAIN' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_INSTRUMENTARIUM DENTAL TWAIN SOFTWARE' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_SOFTWARE' :  'SOFTWARE',
'INOR SOFTWARE_SOFTWARE_VIXWIN' :  'SOFTWARE',
'IO X-RAY_GENDEX IO X-RAY_EXPERT DC' :  'PRODUCT',
'IO X-RAY_INSTRUMENTARIUM IO X-RAY_FOCUS' :  'PRODUCT',
'IO X-RAY_NOMAD IO X-RAY_NOMAD PRO' :  'PRODUCT',
'IO X-RAY_NOMAD IO X-RAY_NOMAD PRO2' :  'PRODUCT',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY 765 DC' :  'PARTS',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY EXPERT DC' :  'PARTS',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY FOCUS' :  'PARTS',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY GX-770' :  'PARTS',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY MINRAY' :  'PARTS',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY ORALIX' :  'PARTS',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PARTS IO X-RAY' :  'PARTS',
'IO-CAMERA_DEXIS IO-CAMERA_CARIVU' :  'PRODUCT',
'IO-CAMERA_DEXIS IO-CAMERA_COMPLETE DEXCAM4 KIT' :  'PRODUCT',
'IO-CAMERA_DEXIS IO-CAMERA_DEXCAM' :  'PRODUCT',
'IO-CAMERA_DEXIS IO-CAMERA_DEXCAM4 HD' :  'PRODUCT',
'IO-CAMERA_GENDEX IO-CAMERA_GXC-300' :  'PRODUCT',
'IO-CAMERA_SPARE PARTS IO-CAMERA_SPARE PART IO-CAMERA DEXCAM' :  'PARTS',
'IO-CAMERA_SPARE PARTS IO-CAMERA_SPARE PARTS IO-CAMERA' :  'PARTS',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS CLASSIC SENSOR' :  'PRODUCT',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS IO-SENSOR' :  'PRODUCT',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS IXS' :  'PRODUCT',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS PLATINUM' :  'PRODUCT',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS TITANIUM' :  'PRODUCT',
'IO-SENSOR_DEXIS IO-SENSOR_FS ERGO' :  'PRODUCT',
'IO-SENSOR_GENDEX IO-SENSOR_GXS-700' :  'PRODUCT',
'IO-SENSOR_INSTRUMENTARIUM IO-SENSOR_SNAPSHOT' :  'PRODUCT',
'IO-SENSOR_IO-SENSOR VISUALIX_VISUALIX' :  'PRODUCT',
'IO-SENSOR_SPARE PARTS IO-SENSOR_SPARE PART IO-SENSOR' :  'PARTS',
'IO-SENSOR_SPARE PARTS IO-SENSOR_SPARE PART IO-SENSOR DEXIS' :  'PARTS',
'IO-SENSOR_SPARE PARTS IO-SENSOR_SPARE PART IO-SENSOR GXS-700' :  'PARTS',
'IO-SENSOR_UPGRD/CONV_DEXIS IXS' :  'UPGRADES',
'IO-SENSOR_UPGRD/CONV_DEXIS PLATINUM' :  'UPGRADES',
'IO-SENSOR_UPGRD/CONV_GXS-700' :  'UPGRADES',
'NAVIGATED SURGERY_X-NAV_CONSUMABLES-NAVIGATED SURGERY' :  'PRODUCT',
'NAVIGATED SURGERY_X-NAV_KITS-X-NAV' :  'PRODUCT',
'NAVIGATED SURGERY_X-NAV_SERVICE' :  'PRODUCT',
'NAVIGATED SURGERY_X-NAV_TOOLING AND LICENSES' :  'PRODUCT',
'NAVIGATED SURGERY_X-NAV_TOOLING AND LlCENSES' :  'PRODUCT',
'NAVIGATED SURGERY_X-NAV_X-GUIDE' :  'PRODUCT',
'SLA_2D-GX-DX-SLA_2D-GX-DX-CORE' :  'SLA',
'SLA_2D-GX-DX-SLA_2D-GX-DX-ESSENTIAL' :  'SLA',
'SLA_2D-GX-DX-SLA_2D-GX-DX-PREMIER' :  'SLA',
'SLA_2D-GX-DX-SLA_2D-GX-DX-UPLIFT' :  'SLA',
'SLA_2D-PX-DX-SLA_2D-PX-DX-CORE' :  'SLA',
'SLA_2D-PX-DX-SLA_2D-PX-DX-ESSENTIAL' :  'SLA',
'SLA_2D-PX-DX-SLA_2D-PX-DX-PREMIER' :  'SLA',
'SLA_2D-PX-DX-SLA_2D-PX-DX-UPLIFT' :  'SLA',
'SLA_3D-GX-DX-SLA_3D-GX-DX-CORE' :  'SLA',
'SLA_3D-GX-DX-SLA_3D-GX-DX-ESSENTIAL' :  'SLA',
'SLA_3D-GX-DX-SLA_3D-GX-DX-PREMIER' :  'SLA',
'SLA_3D-GX-DX-SLA_3D-GX-DX-UPLIFT' :  'SLA',
'SLA_3D-ISI-SLA_3D-ISI-CORE' :  'SLA',
'SLA_3D-ISI-SLA_3D-ISI-ESSENTIAL' :  'SLA',
'SLA_3D-ISI-SLA_3D-ISI-LIMITED' :  'SLA',
'SLA_3D-ISI-SLA_3D-ISI-PREMIER' :  'SLA',
'SLA_3D-ISI-SLA_3D-ISI-UPLIFT' :  'SLA',
'SLA_3D-PX-DX-SLA_3D-PX-DX-CORE' :  'SLA',
'SLA_3D-PX-DX-SLA_3D-PX-DX-ESSEN' :  'SLA',
'SLA_3D-PX-DX-SLA_3D-PX-DX-PREM' :  'SLA',
'SLA_3D-PX-DX-SLA_3D-PX-DX-UPLIFT' :  'SLA',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-CORE' :  'SLA',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-ESSEN' :  'SLA',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-FEES' :  'SLA',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-PREM' :  'SLA',
'SLA_DEXIS-SLA_CARIVU CORE MULTI' :  'SLA',
'SLA_DEXIS-SLA_CARIVU ESSENTIAL' :  'SLA',
'SLA_DEXIS-SLA_CARIVU PREMIER' :  'SLA',
'SLA_DEXIS-SLA_DEXCAM CORE MULTI' :  'SLA',
'SLA_DEXIS-SLA_DEXCAM ESSENTIAL' :  'SLA',
'SLA_DEXIS-SLA_DEXCAM PREMIER' :  'SLA',
'SLA_DEXIS-SLA_DEXIS CORE MULTI' :  'SLA',
'SLA_DEXIS-SLA_DEXIS SLA FEES' :  'SLA',
'SLA_DEXIS-SLA_DEXIS SOFTWARE-SLA' :  'SLA',
'SLA_DEXIS-SLA_DX-SENSOR-ESSENTIAL' :  'SLA',
'SLA_DEXIS-SLA_DX-SENSOR-PREMIER' :  'SLA',
'SLA_NOMAD-SLA_NOMAD BASIC' :  'SLA',
'SLA_NOMAD-SLA_NOMAD ESSENTIAL' :  'SLA',
'SLA_NOMAD-SLA_NOMAD-PREMIER' :  'SLA',
'SLA_SLA_SLA' :  'SLA',
'SOFTWARE_3RDPARTY_SOFTWARE-3RDPARTY INVIVO' :  'SOFTWARE',
'SOFTWARE_DTX STUDIO CLINIC_DTX STUDIO CLINIC PRO + IMPLANT' :  'SOFTWARE',
'SOFTWARE_DTX STUDIO SUITE_DTX STUDIO CLINIC' :  'SOFTWARE',
'SOFTWARE_SOFTWARE 3D_DATAGRABBER FOR 3D' :  'SOFTWARE',
'SOFTWARE_SOFTWARE 3D_SOFTWARE 3D' :  'SOFTWARE',
'SOFTWARE_SOFTWARE 3D_SOREDEX SCANORA 6.X SOFTWARE' :  'SOFTWARE',
'SOFTWARE_SOFTWARE_CS MODEL' :  'SOFTWARE',
'SOFTWARE_SOFTWARE_CS MODEL+' :  'SOFTWARE',
'SOFTWARE_SOFTWARE_CS SCANFLOW' :  'SOFTWARE',
'SOFTWARE_SOFTWARE_LEGACY CS SOFTWARE' :  'SOFTWARE',
'SOFTWARE_SOREDEX SOFTWARE_SOFTWARE-SOREDEX SCANORA' :  'SOFTWARE',
'SPARE PART_SPARE PART_SPARE PART' :  'PARTS',
'Spare Part_Spare Part_SPARE PARTS' :  'PARTS',
'SPARE PARTS_SPARE PARTS_SPARE PARTS' :  'PARTS',
}

catMap = {
'2D_DEXIS 2D_DEXIS OP 2D PAN' :  '2D',
'2D_DEXIS 2D_DEXIS OP 3D PAN UPGRADABLE' :  '2D',
'2D_DEXIS 2D_DEXIS OP 3D PAN/CEPH UPGRADABLE' :  '2D',
'2D_DEXIS 2D_DEXIS OP 3D PRO 2D PAN' :  '2D',
'2D_DEXIS 2D_DEXIS OP 3D PRO 2D PAN/CEPH' :  '2D',
'2D_DEXIS 2D_DEXIS OP 3D PRO UPGRADE' :  '2D',
'2D_GENDEX 2D_GXDP-300 PAN' :  '2D',
'2D_GENDEX 2D_GXDP-700 PAN' :  '2D',
'2D_GENDEX 2D_GXDP-700 PAN/CEPH' :  '2D',
'2D_GENDEX 2D_GXDP-700 UPGRADE 2D' :  '2D',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OC200D PAN/CEPH' :  '2D',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP200D PAN' :  '2D',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP200D UPGRADE TO CEPH' :  '2D',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP30' :  '2D',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP300 PAN' :  '2D',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP300 PAN/CEPH' :  '2D',
'2D_SOFTWARE 2D_SOFTWARE 2D' :  'OTHER',
'2D_SOREDEX 2D_CRANEX D  CEPH' :  '2D',
'2D_SOREDEX 2D_CRANEX NOVUS E' :  '2D',
'2D_SPARE PARTS_SPARE PARTS 2D' :  '2D',
'2D_UPGRD/CONV_DEXIS OP 3D PRO UPGRADE' :  '2D',
'2D_UPGRD/CONV_INSTRUMENTARIUM OP200D UPGRADE TO CEPH' :  '2D',
'2D_UPGRD/CONV_SOFTWARE 2D' :  'OTHER',
'3D_DEXIS 3D_DEXIS OP 3D' :  '3D',
'3D_DEXIS 3D_DEXIS OP 3D LX PAN' :  '3D',
'3D_DEXIS 3D_DEXIS OP 3D PAN' :  '3D',
'3D_DEXIS 3D_DEXIS OP 3D PAN/CEPH' :  '3D',
'3D_DEXIS 3D_DEXIS OP 3D PRO' :  '3D',
'3D_DEXIS 3D_DEXIS OP 3D PRO MFOV PAN' :  '3D',
'3D_DEXIS 3D_DEXIS OP 3D PRO MFOV PAN/CEPH' :  '3D',
'3D_DEXIS 3D_DEXIS OP 3D PRO SFOV' :  '3D',
'3D_DEXIS 3D_DEXIS OP 3D PRO SFOV CEPH' :  '3D',
'3D_GENDEX 3D_GXDP-700 6X4  / 6X8 SFOV' :  '3D',
'3D_GENDEX 3D_GXDP-700 UPGRADE' :  '3D',
'3D_GENDEX 3D_GXDP-700 UPGRADE TO CEPH' :  '3D',
'3D_I-CAT 3D_HW LEGACY CONVKIT' :  '3D',
'3D_I-CAT 3D_I-CAT FLX V10' :  '3D',
'3D_I-CAT 3D_I-CAT FLX V17' :  '3D',
'3D_I-CAT 3D_I-CAT FLX V8' :  '3D',
'3D_I-CAT 3D_I-CAT NEXT-GEN LFOV' :  '3D',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 6X8 SFOV PAN' :  '3D',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 6X8 UPGRADE TO 8X15 MFOV' :  '3D',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO  PAN' :  '3D',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO 13X15 MFOV' :  '3D',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO 8X15 MFOV' :  '3D',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO UPGRADE 8X15 TO 13X15 LFOV' :  '3D',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 UPGRADE PAN TO 8X15 MFOV' :  '3D',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 UPGRADE TO CEPH CEPH' :  '3D',
'3D_SOREDEX 3D_CRANEX 3D' :  'OTHER',
'3D_SOREDEX 3D_CRANEX 3D 6X4 / 6X8 SFOV/CEPH' :  'OTHER',
'3D_SOREDEX 3D_CRANEX 3D UPGRADE PAN TO 3D 6X8 SFOV' :  '3D',
'3D_SPARE PARTS 3D_SPARE PARTS 3D' :  '3D',
'3D_SPARE PARTS I-CAT_SPARE PARTS I-CAT' :  '3D',
'3D_UPGRD/CONV_CRANEX 3D' :  '3D',
'3D_UPGRD/CONV_DEXIS OP 3D PRO CEPH' :  '3D',
'3D_UPGRD/CONV_DEXIS OP 3D UPGRADE' :  '3D',
'3D_UPGRD/CONV_GXDP-700 UPGRADE TO CEPH' :  '3D',
'3D_UPGRD/CONV_I-CAT FLX V17' :  '3D',
'3D_UPGRD/CONV_INSTRUMENTARIUM OP300 MAXIO UPGRADE 8X15 TO 13X15 LFOV' :  '3D',
'ACCESSORIES_ACCESSORIES_CDCM-ACCESSORIES' :  'OTHER',
'CR-READER_GENDEX CR-READER_DENOPTIX' :  'OTHER',
'CR-READER_GENDEX CR-READER_GXPS-500' :  'CR_READER',
'CR-READER_INSTRUMENTARIUM CR-READER_OPTIME' :  'CR_READER',
'CR-READER_SPARE PARTS CR-READER_SPARE PART CR-READER DENOPTIX' :  'OTHER',
'CR-READER_SPARE PARTS CR-READER_SPARE PART CR-READER GXPS-500' :  'OTHER',
'CR-READER_SPARE PARTS CR-READER_SPARE PART CR-READER OPTIME' :  'OTHER',
'CR-READER_SPARE PARTS CR-READER_SPARE PARTS CR-READER' :  'OTHER',
'INOR SOFTWARE_SOFTWARE_CLINIVIEW' :  'CLINIVIEW',
'INOR SOFTWARE_SOFTWARE_CLINIVIEW 11' :  'CLINIVIEW',
'INOR SOFTWARE_SOFTWARE_DEXCLAIM' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIMAGE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIMPLANT' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS IMAGING SUITE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS INTEGRATOR DENTRIX' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS INTEGRATOR EZDENTAL' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS PREMIUM EAGLESOFT INTEGRATOR' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS SOFTWARE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXNET' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXPAN' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXPSP' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXRAY' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXSCAN' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXVOICE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXWRITE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DX IS' :  'OTHER',
'INOR SOFTWARE_SOFTWARE_GXTWAIN' :  'CLINIVIEW',
'INOR SOFTWARE_SOFTWARE_INSTRUMENTARIUM DENTAL TWAIN SOFTWARE' :  'OTHER',
'INOR SOFTWARE_SOFTWARE_SOFTWARE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_VIXWIN' :  'VIXWIN',
'IO X-RAY_GENDEX IO X-RAY_EXPERT DC' :  'NOMAD',
'IO X-RAY_INSTRUMENTARIUM IO X-RAY_FOCUS' :  'NOMAD',
'IO X-RAY_NOMAD IO X-RAY_NOMAD PRO' :  'NOMAD',
'IO X-RAY_NOMAD IO X-RAY_NOMAD PRO2' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY 765 DC' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY EXPERT DC' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY FOCUS' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY GX-770' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY MINRAY' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY ORALIX' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PARTS IO X-RAY' :  'NOMAD',
'IO-CAMERA_DEXIS IO-CAMERA_CARIVU' :  'OTHER',
'IO-CAMERA_DEXIS IO-CAMERA_COMPLETE DEXCAM4 KIT' :  'OTHER',
'IO-CAMERA_DEXIS IO-CAMERA_DEXCAM' :  'OTHER',
'IO-CAMERA_DEXIS IO-CAMERA_DEXCAM4 HD' :  'CAMERA',
'IO-CAMERA_GENDEX IO-CAMERA_GXC-300' :  'OTHER',
'IO-CAMERA_SPARE PARTS IO-CAMERA_SPARE PART IO-CAMERA DEXCAM' :  'OTHER',
'IO-CAMERA_SPARE PARTS IO-CAMERA_SPARE PARTS IO-CAMERA' :  'OTHER',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS CLASSIC SENSOR' :  'SENSOR',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS IO-SENSOR' :  'SENSOR',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS IXS' :  'SENSOR',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS PLATINUM' :  'SENSOR',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS TITANIUM' :  'SENSOR',
'IO-SENSOR_DEXIS IO-SENSOR_FS ERGO' :  'SENSOR',
'IO-SENSOR_GENDEX IO-SENSOR_GXS-700' :  'SENSOR',
'IO-SENSOR_INSTRUMENTARIUM IO-SENSOR_SNAPSHOT' :  'SENSOR',
'IO-SENSOR_IO-SENSOR VISUALIX_VISUALIX' :  'SENSOR',
'IO-SENSOR_SPARE PARTS IO-SENSOR_SPARE PART IO-SENSOR' :  'SENSOR',
'IO-SENSOR_SPARE PARTS IO-SENSOR_SPARE PART IO-SENSOR DEXIS' :  'SENSOR',
'IO-SENSOR_SPARE PARTS IO-SENSOR_SPARE PART IO-SENSOR GXS-700' :  'SENSOR',
'IO-SENSOR_UPGRD/CONV_DEXIS IXS' :  'SENSOR',
'IO-SENSOR_UPGRD/CONV_DEXIS PLATINUM' :  'SENSOR',
'IO-SENSOR_UPGRD/CONV_GXS-700' :  'SENSOR',
'NAVIGATED SURGERY_X-NAV_CONSUMABLES-NAVIGATED SURGERY' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_KITS-X-NAV' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_SERVICE' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_TOOLING AND LICENSES' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_TOOLING AND LlCENSES' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_X-GUIDE' :  'OTHER',
'SLA_2D-GX-DX-SLA_2D-GX-DX-CORE' :  '2D',
'SLA_2D-GX-DX-SLA_2D-GX-DX-ESSENTIAL' :  '2D',
'SLA_2D-GX-DX-SLA_2D-GX-DX-PREMIER' :  '2D',
'SLA_2D-GX-DX-SLA_2D-GX-DX-UPLIFT' :  '2D',
'SLA_2D-PX-DX-SLA_2D-PX-DX-CORE' :  '2D',
'SLA_2D-PX-DX-SLA_2D-PX-DX-ESSENTIAL' :  '2D',
'SLA_2D-PX-DX-SLA_2D-PX-DX-PREMIER' :  '2D',
'SLA_2D-PX-DX-SLA_2D-PX-DX-UPLIFT' :  '2D',
'SLA_3D-GX-DX-SLA_3D-GX-DX-CORE' :  '3D',
'SLA_3D-GX-DX-SLA_3D-GX-DX-ESSENTIAL' :  '3D',
'SLA_3D-GX-DX-SLA_3D-GX-DX-PREMIER' :  '3D',
'SLA_3D-GX-DX-SLA_3D-GX-DX-UPLIFT' :  '3D',
'SLA_3D-ISI-SLA_3D-ISI-CORE' :  '3D',
'SLA_3D-ISI-SLA_3D-ISI-ESSENTIAL' :  '3D',
'SLA_3D-ISI-SLA_3D-ISI-LIMITED' :  '3D',
'SLA_3D-ISI-SLA_3D-ISI-PREMIER' :  '3D',
'SLA_3D-ISI-SLA_3D-ISI-UPLIFT' :  '3D',
'SLA_3D-PX-DX-SLA_3D-PX-DX-CORE' :  '3D',
'SLA_3D-PX-DX-SLA_3D-PX-DX-ESSEN' :  '3D',
'SLA_3D-PX-DX-SLA_3D-PX-DX-PREM' :  '3D',
'SLA_3D-PX-DX-SLA_3D-PX-DX-UPLIFT' :  '3D',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-CORE' :  'SENSOR',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-ESSEN' :  'SENSOR',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-FEES' :  'SENSOR',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-PREM' :  'SENSOR',
'SLA_DEXIS-SLA_CARIVU CORE MULTI' :  'SENSOR',
'SLA_DEXIS-SLA_CARIVU ESSENTIAL' :  'SENSOR',
'SLA_DEXIS-SLA_CARIVU PREMIER' :  'SENSOR',
'SLA_DEXIS-SLA_DEXCAM CORE MULTI' :  'SENSOR',
'SLA_DEXIS-SLA_DEXCAM ESSENTIAL' :  'SENSOR',
'SLA_DEXIS-SLA_DEXCAM PREMIER' :  'SENSOR',
'SLA_DEXIS-SLA_DEXIS CORE MULTI' :  'SENSOR',
'SLA_DEXIS-SLA_DEXIS SLA FEES' :  'SENSOR',
'SLA_DEXIS-SLA_DEXIS SOFTWARE-SLA' :  'SOFTWARE',
'SLA_DEXIS-SLA_DX-SENSOR-ESSENTIAL' :  'SENSOR',
'SLA_DEXIS-SLA_DX-SENSOR-PREMIER' :  'SENSOR',
'SLA_NOMAD-SLA_NOMAD BASIC' :  'NOMAD',
'SLA_NOMAD-SLA_NOMAD ESSENTIAL' :  'NOMAD',
'SLA_NOMAD-SLA_NOMAD-PREMIER' :  'NOMAD',
'SLA_SLA_SLA' :  'OTHER',
'SOFTWARE_3RDPARTY_SOFTWARE-3RDPARTY INVIVO' :  'INVIVO',
'SOFTWARE_DTX STUDIO CLINIC_DTX STUDIO CLINIC PRO + IMPLANT' :  'DEXIS_SOFTWARE',
'SOFTWARE_DTX STUDIO SUITE_DTX STUDIO CLINIC' :  'DTX',
'SOFTWARE_SOFTWARE 3D_DATAGRABBER FOR 3D' :  'CLINIVIEW',
'SOFTWARE_SOFTWARE 3D_SOFTWARE 3D' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOFTWARE 3D_SOREDEX SCANORA 6.X SOFTWARE' :  'CLINIVIEW',
'SOFTWARE_SOFTWARE_CS MODEL' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOFTWARE_CS MODEL+' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOFTWARE_CS SCANFLOW' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOFTWARE_LEGACY CS SOFTWARE' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOREDEX SOFTWARE_SOFTWARE-SOREDEX SCANORA' :  'CLINIVIEW',
'SPARE PART_SPARE PART_SPARE PART' :  'OTHER',
'Spare Part_Spare Part_SPARE PARTS' :  'OTHER',
'SPARE PARTS_SPARE PARTS_SPARE PARTS' :  'OTHER',
}

subMap = {
'2D_DEXIS 2D_DEXIS OP 2D PAN' :  'PAN',
'2D_DEXIS 2D_DEXIS OP 3D PAN UPGRADABLE' :  'PAN',
'2D_DEXIS 2D_DEXIS OP 3D PAN/CEPH UPGRADABLE' :  'PAN',
'2D_DEXIS 2D_DEXIS OP 3D PRO 2D PAN' :  'PAN',
'2D_DEXIS 2D_DEXIS OP 3D PRO 2D PAN/CEPH' :  'CEPH',
'2D_DEXIS 2D_DEXIS OP 3D PRO UPGRADE' :  '2D',
'2D_GENDEX 2D_GXDP-300 PAN' :  'PAN',
'2D_GENDEX 2D_GXDP-700 PAN' :  'PAN',
'2D_GENDEX 2D_GXDP-700 PAN/CEPH' :  'CEPH',
'2D_GENDEX 2D_GXDP-700 UPGRADE 2D' :  '2D',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OC200D PAN/CEPH' :  'CEPH',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP200D PAN' :  'PAN',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP200D UPGRADE TO CEPH' :  '2D',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP30' :  'PAN',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP300 PAN' :  'PAN',
'2D_INSTRUMENTARIUM 2D_INSTRUMENTARIUM OP300 PAN/CEPH' :  'CEPH',
'2D_SOFTWARE 2D_SOFTWARE 2D' :  'OTHER',
'2D_SOREDEX 2D_CRANEX D  CEPH' :  'CEPH',
'2D_SOREDEX 2D_CRANEX NOVUS E' :  'PAN',
'2D_SPARE PARTS_SPARE PARTS 2D' :  '2D',
'2D_UPGRD/CONV_DEXIS OP 3D PRO UPGRADE' :  '2D',
'2D_UPGRD/CONV_INSTRUMENTARIUM OP200D UPGRADE TO CEPH' :  '2D',
'2D_UPGRD/CONV_SOFTWARE 2D' :  'OTHER',
'3D_DEXIS 3D_DEXIS OP 3D' :  'RENEW_DIG',
'3D_DEXIS 3D_DEXIS OP 3D LX PAN' :  'CBCT',
'3D_DEXIS 3D_DEXIS OP 3D PAN' :  'CBCT',
'3D_DEXIS 3D_DEXIS OP 3D PAN/CEPH' :  'CBCT_CEPH',
'3D_DEXIS 3D_DEXIS OP 3D PRO' :  'CBCT',
'3D_DEXIS 3D_DEXIS OP 3D PRO MFOV PAN' :  'CBCT',
'3D_DEXIS 3D_DEXIS OP 3D PRO MFOV PAN/CEPH' :  'CBCT_CEPH',
'3D_DEXIS 3D_DEXIS OP 3D PRO SFOV' :  'CBCT',
'3D_DEXIS 3D_DEXIS OP 3D PRO SFOV CEPH' :  'CBCT_CEPH',
'3D_GENDEX 3D_GXDP-700 6X4  / 6X8 SFOV' :  'CBCT',
'3D_GENDEX 3D_GXDP-700 UPGRADE' :  '3D',
'3D_GENDEX 3D_GXDP-700 UPGRADE TO CEPH' :  'CBCT_CEPH',
'3D_I-CAT 3D_HW LEGACY CONVKIT' :  'I_CAT',
'3D_I-CAT 3D_I-CAT FLX V10' :  'I_CAT',
'3D_I-CAT 3D_I-CAT FLX V17' :  'I_CAT',
'3D_I-CAT 3D_I-CAT FLX V8' :  'I_CAT',
'3D_I-CAT 3D_I-CAT NEXT-GEN LFOV' :  'I_CAT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 6X8 SFOV PAN' :  'CBCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 6X8 UPGRADE TO 8X15 MFOV' :  '3D',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO  PAN' :  'CBCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO 13X15 MFOV' :  'CBCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO 8X15 MFOV' :  'CBCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 MAXIO UPGRADE 8X15 TO 13X15 LFOV' :  'CBCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 UPGRADE PAN TO 8X15 MFOV' :  'CBCT',
'3D_INSTRUMENTARIUM 3D_INSTRUMENTARIUM OP300 UPGRADE TO CEPH CEPH' :  '3D',
'3D_SOREDEX 3D_CRANEX 3D' :  'CRANEX_3D',
'3D_SOREDEX 3D_CRANEX 3D 6X4 / 6X8 SFOV/CEPH' :  'OTHER',
'3D_SOREDEX 3D_CRANEX 3D UPGRADE PAN TO 3D 6X8 SFOV' :  '3D',
'3D_SPARE PARTS 3D_SPARE PARTS 3D' :  '3D',
'3D_SPARE PARTS I-CAT_SPARE PARTS I-CAT' :  '3D',
'3D_UPGRD/CONV_CRANEX 3D' :  '3D',
'3D_UPGRD/CONV_DEXIS OP 3D PRO CEPH' :  '3D',
'3D_UPGRD/CONV_DEXIS OP 3D UPGRADE' :  '3D',
'3D_UPGRD/CONV_GXDP-700 UPGRADE TO CEPH' :  '3D',
'3D_UPGRD/CONV_I-CAT FLX V17' :  '3D',
'3D_UPGRD/CONV_INSTRUMENTARIUM OP300 MAXIO UPGRADE 8X15 TO 13X15 LFOV' :  '3D',
'ACCESSORIES_ACCESSORIES_CDCM-ACCESSORIES' :  'OTHER',
'CR-READER_GENDEX CR-READER_DENOPTIX' :  'DENOPTIX',
'CR-READER_GENDEX CR-READER_GXPS-500' :  'GXPS_500',
'CR-READER_INSTRUMENTARIUM CR-READER_OPTIME' :  'OPTIME',
'CR-READER_SPARE PARTS CR-READER_SPARE PART CR-READER DENOPTIX' :  'OTHER',
'CR-READER_SPARE PARTS CR-READER_SPARE PART CR-READER GXPS-500' :  'OTHER',
'CR-READER_SPARE PARTS CR-READER_SPARE PART CR-READER OPTIME' :  'OTHER',
'CR-READER_SPARE PARTS CR-READER_SPARE PARTS CR-READER' :  'OTHER',
'INOR SOFTWARE_SOFTWARE_CLINIVIEW' :  'CLINIVIEW',
'INOR SOFTWARE_SOFTWARE_CLINIVIEW 11' :  'CLINIVIEW',
'INOR SOFTWARE_SOFTWARE_DEXCLAIM' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIMAGE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIMPLANT' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS IMAGING SUITE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS INTEGRATOR DENTRIX' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS INTEGRATOR EZDENTAL' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS PREMIUM EAGLESOFT INTEGRATOR' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXIS SOFTWARE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXNET' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXPAN' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXPSP' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXRAY' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXSCAN' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXVOICE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DEXWRITE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_DX IS' :  'OTHER',
'INOR SOFTWARE_SOFTWARE_GXTWAIN' :  'CLINIVIEW',
'INOR SOFTWARE_SOFTWARE_INSTRUMENTARIUM DENTAL TWAIN SOFTWARE' :  'OTHER',
'INOR SOFTWARE_SOFTWARE_SOFTWARE' :  'DEXIS_SOFTWARE',
'INOR SOFTWARE_SOFTWARE_VIXWIN' :  'VIXWIN',
'IO X-RAY_GENDEX IO X-RAY_EXPERT DC' :  'PRO_2',
'IO X-RAY_INSTRUMENTARIUM IO X-RAY_FOCUS' :  'PRO_2',
'IO X-RAY_NOMAD IO X-RAY_NOMAD PRO' :  'PRO_2',
'IO X-RAY_NOMAD IO X-RAY_NOMAD PRO2' :  'PRO_2',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY 765 DC' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY EXPERT DC' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY FOCUS' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY GX-770' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY MINRAY' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PART IO X-RAY ORALIX' :  'NOMAD',
'IO X-RAY_SPARE PARTS IO X-RAY_SPARE PARTS IO X-RAY' :  'NOMAD',
'IO-CAMERA_DEXIS IO-CAMERA_CARIVU' :  'CARIVU',
'IO-CAMERA_DEXIS IO-CAMERA_COMPLETE DEXCAM4 KIT' :  'OTHER',
'IO-CAMERA_DEXIS IO-CAMERA_DEXCAM' :  'DEXCAM',
'IO-CAMERA_DEXIS IO-CAMERA_DEXCAM4 HD' :  'DEXCAM4',
'IO-CAMERA_GENDEX IO-CAMERA_GXC-300' :  'OTHER',
'IO-CAMERA_SPARE PARTS IO-CAMERA_SPARE PART IO-CAMERA DEXCAM' :  'OTHER',
'IO-CAMERA_SPARE PARTS IO-CAMERA_SPARE PARTS IO-CAMERA' :  'OTHER',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS CLASSIC SENSOR' :  'IXS',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS IO-SENSOR' :  'TITANIUM',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS IXS' :  'IXS',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS PLATINUM' :  'TITANIUM',
'IO-SENSOR_DEXIS IO-SENSOR_DEXIS TITANIUM' :  'TITANIUM',
'IO-SENSOR_DEXIS IO-SENSOR_FS ERGO' :  'ERGO',
'IO-SENSOR_GENDEX IO-SENSOR_GXS-700' :  'IXS',
'IO-SENSOR_INSTRUMENTARIUM IO-SENSOR_SNAPSHOT' :  'IXS',
'IO-SENSOR_IO-SENSOR VISUALIX_VISUALIX' :  'IXS',
'IO-SENSOR_SPARE PARTS IO-SENSOR_SPARE PART IO-SENSOR' :  'SENSOR',
'IO-SENSOR_SPARE PARTS IO-SENSOR_SPARE PART IO-SENSOR DEXIS' :  'SENSOR',
'IO-SENSOR_SPARE PARTS IO-SENSOR_SPARE PART IO-SENSOR GXS-700' :  'SENSOR',
'IO-SENSOR_UPGRD/CONV_DEXIS IXS' :  'IXS',
'IO-SENSOR_UPGRD/CONV_DEXIS PLATINUM' :  'TITANIUM',
'IO-SENSOR_UPGRD/CONV_GXS-700' :  'IXS',
'NAVIGATED SURGERY_X-NAV_CONSUMABLES-NAVIGATED SURGERY' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_KITS-X-NAV' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_SERVICE' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_TOOLING AND LICENSES' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_TOOLING AND LlCENSES' :  'OTHER',
'NAVIGATED SURGERY_X-NAV_X-GUIDE' :  'X_GUIDE',
'SLA_2D-GX-DX-SLA_2D-GX-DX-CORE' :  'CORE',
'SLA_2D-GX-DX-SLA_2D-GX-DX-ESSENTIAL' :  'ESSENTIAL',
'SLA_2D-GX-DX-SLA_2D-GX-DX-PREMIER' :  'PREMIER',
'SLA_2D-GX-DX-SLA_2D-GX-DX-UPLIFT' :  'UPLIFT',
'SLA_2D-PX-DX-SLA_2D-PX-DX-CORE' :  'CORE',
'SLA_2D-PX-DX-SLA_2D-PX-DX-ESSENTIAL' :  'ESSENTIAL',
'SLA_2D-PX-DX-SLA_2D-PX-DX-PREMIER' :  'PREMIER',
'SLA_2D-PX-DX-SLA_2D-PX-DX-UPLIFT' :  'UPLIFT',
'SLA_3D-GX-DX-SLA_3D-GX-DX-CORE' :  'CORE',
'SLA_3D-GX-DX-SLA_3D-GX-DX-ESSENTIAL' :  'ESSENTIAL',
'SLA_3D-GX-DX-SLA_3D-GX-DX-PREMIER' :  'PREMIER',
'SLA_3D-GX-DX-SLA_3D-GX-DX-UPLIFT' :  'UPLIFT',
'SLA_3D-ISI-SLA_3D-ISI-CORE' :  'CORE',
'SLA_3D-ISI-SLA_3D-ISI-ESSENTIAL' :  'ESSENTIAL',
'SLA_3D-ISI-SLA_3D-ISI-LIMITED' :  'OTHER',
'SLA_3D-ISI-SLA_3D-ISI-PREMIER' :  'PREMIER',
'SLA_3D-ISI-SLA_3D-ISI-UPLIFT' :  'UPLIFT',
'SLA_3D-PX-DX-SLA_3D-PX-DX-CORE' :  'CORE',
'SLA_3D-PX-DX-SLA_3D-PX-DX-ESSEN' :  'ESSENTIAL',
'SLA_3D-PX-DX-SLA_3D-PX-DX-PREM' :  'PREMIER',
'SLA_3D-PX-DX-SLA_3D-PX-DX-UPLIFT' :  'UPLIFT',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-CORE' :  'CORE',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-ESSEN' :  'ESSENTIAL',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-FEES' :  'FEES',
'SLA_DEXIS-GX-SLA_DX-GX-SENSOR-PREM' :  'PREMIER',
'SLA_DEXIS-SLA_CARIVU CORE MULTI' :  'CORE',
'SLA_DEXIS-SLA_CARIVU ESSENTIAL' :  'ESSENTIAL',
'SLA_DEXIS-SLA_CARIVU PREMIER' :  'PREMIER',
'SLA_DEXIS-SLA_DEXCAM CORE MULTI' :  'CORE',
'SLA_DEXIS-SLA_DEXCAM ESSENTIAL' :  'ESSENTIAL',
'SLA_DEXIS-SLA_DEXCAM PREMIER' :  'PREMIER',
'SLA_DEXIS-SLA_DEXIS CORE MULTI' :  'CORE',
'SLA_DEXIS-SLA_DEXIS SLA FEES' :  'FEES',
'SLA_DEXIS-SLA_DEXIS SOFTWARE-SLA' :  'SOFTWARE',
'SLA_DEXIS-SLA_DX-SENSOR-ESSENTIAL' :  'ESSENTIAL',
'SLA_DEXIS-SLA_DX-SENSOR-PREMIER' :  'PREMIER',
'SLA_NOMAD-SLA_NOMAD BASIC' :  'ESSENTIAL',
'SLA_NOMAD-SLA_NOMAD ESSENTIAL' :  'ESSENTIAL',
'SLA_NOMAD-SLA_NOMAD-PREMIER' :  'PREMIER',
'SLA_SLA_SLA' :  'FEES',
'SOFTWARE_3RDPARTY_SOFTWARE-3RDPARTY INVIVO' :  'INVIVO',
'SOFTWARE_DTX STUDIO CLINIC_DTX STUDIO CLINIC PRO + IMPLANT' :  'DEXIS_SOFTWARE',
'SOFTWARE_DTX STUDIO SUITE_DTX STUDIO CLINIC' :  'DTX',
'SOFTWARE_SOFTWARE 3D_DATAGRABBER FOR 3D' :  'CLINIVIEW',
'SOFTWARE_SOFTWARE 3D_SOFTWARE 3D' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOFTWARE 3D_SOREDEX SCANORA 6.X SOFTWARE' :  'CLINIVIEW',
'SOFTWARE_SOFTWARE_CS MODEL' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOFTWARE_CS MODEL+' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOFTWARE_CS SCANFLOW' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOFTWARE_LEGACY CS SOFTWARE' :  'DEXIS_SOFTWARE',
'SOFTWARE_SOREDEX SOFTWARE_SOFTWARE-SOREDEX SCANORA' :  'CLINIVIEW',
'SPARE PART_SPARE PART_SPARE PART' :  'OTHER',
'Spare Part_Spare Part_SPARE PARTS' :  'OTHER',
'SPARE PARTS_SPARE PARTS_SPARE PARTS' :  'OTHER',

}

rename = {
'DIVISION':'DIVISION', 
'PRODUCT':'PRODUCT',
'Prod Cat Commission':'COMMISSIONABLE', 
'REGION':'REGION', 
'TERR':'TERR', 
'3DIGIT':'3DIGIT',    
'SHIP_MDM_SDS_CUST_CLASS1':'CUST_CLASS_1', 
'SHIP_MDM_SDS_CUST_CLASS2':'CUST_CLASS_2',
'SHIP_MDM_SDS_CUST_CLASS3':'CUST_CLASS_3', 
'B2 Major Dealer' : 'DEALER'
}

segMap = {
'STANDARD_STANDARD_STANDARD' : 'STANDARD',
'SPECIAL MARKETS_DSO_SM DSO' : 'MIDTIER_DSO',
'SPECIAL MARKETS_DSO_ELITE DSO' : 'ELITE_DSO',
'INSTITUTION_COMMUNITY HEALTHCARE_COMMUNITY HEALTHCARE' : 'CHC',
'NaN' : 'STANDARD',
'INSTITUTION_SCHOOLS_UNIVERSITY' : 'UNIVERSITY',
'INSTITUTION_GOVERNMENT_INDIAN HEALTH' : 'INDIAN_HEALTH',
'INSTITUTION_SCHOOLS_TECHNICAL SCHOOL' : 'TECHNICAL_SCHOOL',
'INSTITUTION_GOVERNMENT_PUBLIC HEALTH' : 'PUBLIC_HEALTH',
'INSTITUTION_GOVERNMENT_UNIFORMED SERVICES' : 'MILITARY',
'STANDARD_EMERGING DSO_EMERGING DSO 3-9' : 'STANDARD',
'INSTITUTION_GOVERNMENT_PRISONS' : 'PRISONS',
"INSTITUTION_GOVERNMENT_VETERAN'S AFFAIRS" : 'VETERAN_AFFAIRS',
'INSTITUTION_GOVERNMENT_FORENSICS' : 'FORENSICS',
'INSTITUTION_GOVERNMENT_GOVERNMENT' : 'GOVERNMENT',
'DEALER_DEALER_DEALER' : 'DEALER',
'STANDARD_STANDARD_PRIVATE PRACTICE' : 'STANDARD',
'STANDARD_EMERGING DSO_EMERGING DSO 10-19' : 'STANDARD',
}

data5['SHIP_MDM_SDS_CUST_CLASS1'] = np.where(data5['SHIP_MDM_SDS_CUST_CLASS3']=='ELITE DSO','SPECIAL MARKETS', data5['SHIP_MDM_SDS_CUST_CLASS1'])
data5['SHIP_MDM_SDS_CUST_CLASS2'] = np.where(data5['SHIP_MDM_SDS_CUST_CLASS3']=='ELITE DSO','DSO', data5['SHIP_MDM_SDS_CUST_CLASS2'])
data5['STOCK_OR_NOT'] = np.where(data5['SHIP_MDM_SDS_CUST_CLASS3']=='ELITE DSO','USER_ORDER', data5['STOCK_OR_NOT'])
data5['div_pre'] = data5['Prod Cat L1']+"_"+data5['Prod Cat L2_V2']
data5['cat_pre'] = data5['div_pre']+"_"+data5['Prod Cat L3']
data5['DIVISION'] = data5['cat_pre'].map(prdMap)
data5['CATEGORY'] = data5['cat_pre'].map(catMap)
data5['SUBCATEGORY'] = data5['cat_pre'].map(subMap)
data5['PRODUCT'] = data5['CATEGORY']+'_'+data5['SUBCATEGORY']

data6 = data5.rename(columns=rename)
data6['SEGMENT_PRE'] = data6['CUST_CLASS_1']+'_'+data6['CUST_CLASS_2']+'_'+data6['CUST_CLASS_3']
data6['SEGMENT_PRE'].fillna('STANDARD_STANDARD_STANDARD')

data6['CUST_SEGMENT'] = data6['SEGMENT_PRE'].map(segMap)

seg1_tsm = (data6['DIVISION']=='PRODUCT')& (data6['CUST_SEGMENT']=='STANDARD')& (data6['COMMISSIONABLE']=='COMM')& (data6['STOCK_OR_NOT']=='USER_ORDER')
seg2_sla = (data6['DIVISION']=='SLA')
seg3_mds = (data6['DIVISION']=='PRODUCT')& (data6['CUST_SEGMENT']=='MIDTIER_DSO')
seg4_ins = (data6['DIVISION']=='PRODUCT')& (data6['CUST_SEGMENT'].isin(['CHC','FORENSICS','INDIAN_HEALTH','MILITARY','PRISONS','PUBLIC_HEALTH','TECHNICAL_SCHOOL','UNIVERSITY','VETERAN_AFFAIRS'])) 
seg5_eds = (data6['DIVISION']=='PRODUCT')& (data6['CUST_SEGMENT']=='ELITE_DSO')& (data6['STOCK_OR_NOT']=='USER_ORDER')
seg6_prt = (data6['DIVISION']=='PARTS')
seg7_sft = (data6['DIVISION']=='SOFTWARE')
seg8_upr = (data6['DIVISION']=='UPGRADES')
seg9_ncm = (data6['DIVISION']=='PRODUCT')& (data6['CUST_SEGMENT']=='STANDARD')& (data6['COMMISSIONABLE']=='NON_COMM')& (data6['STOCK_OR_NOT']=='USER_ORDER')
seg10_stk = (data6['DIVISION']=='PRODUCT')& (data6['CUST_SEGMENT']=='STANDARD')& (data6['STOCK_OR_NOT']=='STOCK')

conditions = [seg1_tsm,seg2_sla,seg3_mds ,seg4_ins,seg5_eds,seg6_prt,seg7_sft,seg8_upr,seg9_ncm,seg10_stk]
values =     ['TSM'   ,'SLA'   ,'MID_DSO','INST'  ,'EDSO'  ,'PRTS'  ,'SFTW'  ,'UPGR'  ,'NCOM'  ,'STCK']
data6['SEG_GRP'] = np.select(conditions,values,default='PRTS')


data6['RESP'] = np.where(data6['SEG_GRP']=='TSM',data6['TSM'],
                         np.where(data6['SEG_GRP']=='SLA',data6['SLA_AM'],
                                  np.where(data6['SEG_GRP']=='MID_DSO','PAUL TAYLOR',
                                           np.where(data6['SEG_GRP']=='INST',data6['CUST_SEGMENT'],
                                                    np.where(data6['SEG_GRP']=='EDSO','ELITE DSO / AMY MCCARTHY',
                                                             np.where(data6['SEG_GRP']=='PRTS','PARTS',
                                                                      np.where(data6['SEG_GRP']=='SFTW','SOFTWARE SALES',
                                                                               np.where(data6['SEG_GRP']=='UPGR','UPGRADES',
                                                                                        np.where(data6['SEG_GRP']=='NCOM','NON-COMM SALES / UNKNOWN',
                                                                                                 np.where(data6['SEG_GRP']=='STCK',data6['DEALER'],"UNKNOWN"))))))))))

data7 = data6.groupby(['445 Year', '445 Quarter',  '445 Month', '445 Week', 'REGION', 'TERR', 'SEG_GRP', 'DIVISION','STOCK_OR_NOT',
       'COMMISSIONABLE', 'PRODUCT', 'CUST_SEGMENT','RESP'],dropna=False).agg({'Net Amount USD':'sum'}).reset_index()

rep = pd.read_excel("C:\\Users\\5019125\\OneDrive - Envista\\General\\Sr Business Analyst Working Papers\\02_Projects\\45_Sell In Stocking v Region\\reps_DEXIS_2.xlsx", sheet_name="Sheet1")
dic = {'ZIP CODE':'3dig', 'STATE':'st_cd', '2022 REGION':'REGION', '2022 RSD':'img_rsd', '2022 TERR':'TERR','2022 TSM':'img_tsm'}
rep = rep.rename(columns=dic)

zipcodes = pd.read_excel('C:\\Users\\5019125\\OneDrive - Envista\\General\\Sr Business Analyst Working Papers\\02_Projects\\49_Forecast Sell In\\ZIP_Locale_Detail.xls'
                         ,dtype={'DELIVERY ZIPCODE':str,'PHYSICAL ZIP':str})
crz = pd.read_excel('C:\\Users\\5019125\\OneDrive - Envista\\General\\Sr Business Analyst Working Papers\\02_Projects\\49_Forecast Sell In\\zip_zcta.xlsx'
                    ,sheet_name='Zip to ZCTA',dtype={'ZIP_CODE':str,'ZCTA':str})
zpc = pd.merge(zipcodes,crz, left_on='DELIVERY ZIPCODE',right_on='ZIP_CODE',how='left')
col7 = ['ZCTA','ZIP_CODE']
zpc = zpc[col7]
zpc = zpc.drop_duplicates(subset=['ZCTA'],keep='first')

col = ['NAME', 'DP05_0001E']

pop22 = pd.read_csv('C:\\Users\\5019125\\OneDrive - Envista\\General\\Sr Business Analyst Working Papers\\02_Projects\\49_Forecast Sell In\\productDownload_2023-08-22T163802\\ACSDP5Y2021.DP05-Data.csv', low_memory=False,usecols=col)
pop22.drop(index=pop22.index[0], axis=0, inplace=True)
pop22['DP05_0001E'] = pop22['DP05_0001E'].astype(np.int64)
pop22 = pop22.rename(columns={'DP05_0001E':'2022'})

pop21 = pd.read_csv('C:\\Users\\5019125\\OneDrive - Envista\\General\\Sr Business Analyst Working Papers\\02_Projects\\49_Forecast Sell In\\productDownload_2023-08-22T163802\\ACSDP5Y2020.DP05-Data.csv', low_memory=False,usecols=col)
pop21.drop(index=pop21.index[0], axis=0, inplace=True)
pop21['DP05_0001E'] = pop21['DP05_0001E'].astype(np.int64)
pop21 = pop21.rename(columns={'DP05_0001E':'2021'})

pop20 = pd.read_csv('C:\\Users\\5019125\\OneDrive - Envista\\General\\Sr Business Analyst Working Papers\\02_Projects\\49_Forecast Sell In\\productDownload_2023-08-22T163802\\ACSDP5Y2019.DP05-Data.csv', low_memory=False,usecols=col)
pop20.drop(index=pop20.index[0], axis=0, inplace=True)
pop20['DP05_0001E'] = pop20['DP05_0001E'].astype(np.int64)
pop20 = pop20.rename(columns={'DP05_0001E':'2020'})

pop19 = pd.read_csv('C:\\Users\\5019125\\OneDrive - Envista\\General\\Sr Business Analyst Working Papers\\02_Projects\\49_Forecast Sell In\\productDownload_2023-08-22T163802\\ACSDP5Y2018.DP05-Data.csv', low_memory=False,usecols=col)
pop19.drop(index=pop19.index[0], axis=0, inplace=True)
pop19['DP05_0001E'] = pop19['DP05_0001E'].astype(np.int64)
pop19 = pop19.rename(columns={'DP05_0001E':'2019'})

pop18 = pd.read_csv('C:\\Users\\5019125\\OneDrive - Envista\\General\\Sr Business Analyst Working Papers\\02_Projects\\49_Forecast Sell In\\productDownload_2023-08-22T163802\\ACSDP5Y2017.DP05-Data.csv', low_memory=False,usecols=col)
pop18.drop(index=pop18.index[0], axis=0, inplace=True)
pop18['DP05_0001E'] = pop18['DP05_0001E'].astype(np.int64)
pop18 = pop18.rename(columns={'DP05_0001E':'2018'})

usPop = pop22.merge(pop21,on='NAME',how='outer')
usPop = usPop.merge(pop20,on='NAME',how='outer')
usPop = usPop.merge(pop19,on='NAME',how='outer')
usPop = usPop.merge(pop18,on='NAME',how='outer')
usPop = usPop.fillna(0)

usPop['2022_Chg'] = usPop['2022']-usPop['2021']
usPop['2021_Chg'] = usPop['2021']-usPop['2020']
usPop['2020_Chg'] = usPop['2020']-usPop['2019']
usPop['2019_Chg'] = usPop['2019']-usPop['2018']

usPop['ZCTA'] = usPop['NAME'].str[-5:]
usPop1 = pd.merge(usPop,zpc, on='ZCTA',how='left')
usPop1['ZIP_CODE'] = usPop1['ZIP_CODE'].fillna(usPop1['ZCTA'],inplace=True)
usPop1['zip3'] = usPop1['ZCTA'].str[-5:3]

col = ['2022', '2021', '2020', '2019', '2018', '2022_Chg', '2021_Chg','2020_Chg', '2019_Chg']
usPop2 = usPop1.groupby(['zip3'])[col].sum().reset_index()

caPop = pd.read_excel('C:\\Users\\5019125\\OneDrive - Envista\\General\\Sr Business Analyst Working Papers\\02_Projects\\49_Forecast Sell In\\caPop.xlsx')

caPop['2022_Chg'] = caPop['2022']-caPop['2021']
caPop['2021_Chg'] = caPop['2021']-caPop['2020']
caPop['2020_Chg'] = caPop['2020']-caPop['2019']
caPop['2019_Chg'] = caPop['2019']-caPop['2018']
caPop = caPop.drop(columns=['2017'])

pop = pd.concat([usPop2,caPop])
# Melt the DataFrame
df_melt = pd.melt(pop, id_vars=['zip3'], value_vars=['2022', '2021', '2020', '2019', '2018'],
                  var_name='year', value_name='population')
df_melt['year'] = df_melt['year'].astype(int)
weights = df_melt['year'].apply(lambda x: 10 if x==2022 else 1)

# Predict 2023 for each ZIP code
predictions = {}
for zipcode in pop['zip3']:
    subset = df_melt[df_melt['zip3'] == zipcode]
    X = subset[['year']].values
    y = subset['population']
    sample_weights = weights[subset.index]

    model = LinearRegression()
    model.fit(X, y, sample_weight=sample_weights)
    predictions[zipcode] = model.predict([[2023]])[0]

df_2023 = pd.DataFrame(list(predictions.items()), columns=['zip3', '2023'])
df_2023['2023']= df_2023['2023'].astype(int)

pop = pd.merge(pop,df_2023, on='zip3', how='inner')
pop['2023_Chg'] = pop['2023']-pop['2022']


pop1 = pd.merge(pop,rep,left_on="zip3",right_on='3dig',how='left')
pop1['TERR'] = pop1['TERR'].str.replace(' - ','_')
pop1['REGION'] = pop1['REGION'].str.replace(' ','_')
pop1 = pop1.sort_values(by='zip3')
pop1['REGION'].fillna(method='ffill', inplace=True)
pop1['TERR'].fillna(method='ffill', inplace=True)


pop1['REG_TERR'] = pop1['REGION']+'_'+pop1['TERR']

pop2 = pop1.groupby('REG_TERR').agg(
        _2023=('2023_Chg', 'sum'),
        _2022=('2022_Chg', 'sum'),
        _2021=('2021_Chg', 'sum'),
        _2020=('2020_Chg', 'sum'),
        _2019=('2019_Chg', 'sum'),
).reset_index()
pop3 = pd.melt(pop2, id_vars=['REG_TERR'], var_name='445 Year',value_vars=[ '_2023','_2022', '_2021', '_2020', '_2019'])
pop3['445 Year'] = pop3['445 Year'].str.replace('_','')

pop3 = pop3.sort_values(by=['REG_TERR','445 Year'])

seg1 = data6[data6['SEG_GRP']=='TSM']
edso = data6[(data6['CUST_CLASS_3']=='ELITE DSO') 
             & (data6['DIVISION']=='PRODUCT') 
             & (data6['COMMISSIONABLE']=='COMM')
             & (data6['CATEGORY']=='SENSOR')]

## Exclude the Items that have more than Sensor in them
snsEx = ['10132916',
'08225013',
'10137113',
'10137115']

snsAsp = edso[(edso['CATEGORY']=='SENSOR') & (edso['Net Amount USD']>0)]
snsAsp = snsAsp[~snsAsp['Product Number'].isin(snsEx)]

snsAsp['asp'] = snsAsp['Net Amount USD']/snsAsp['Net QTY']
snsAsp = snsAsp.sort_values(by='445 Year')

# Compute the summary statistics
summary_stats1 = snsAsp.groupby('445 Year').agg(
    min_price1=('asp', 'min'),
    max_price1=('asp', 'max'),
    median_price1=('asp', 'median'),
    first_quartile1=('asp', lambda x: x.quantile(0.25)),
    third_quartile1=('asp', lambda x: x.quantile(0.75)),
    mean_price1=('asp', 'mean'),
    std_dev1=('asp', np.std)
).reset_index()

# Merge the summary stats back to the original dataframe
snsAsp2= snsAsp.merge(summary_stats1, on="445 Year", how="left")

# # Compute how many standard deviations each price is away from the mean
snsAsp2["std_dev_away1"] = (snsAsp2["asp"] - snsAsp2["mean_price1"]) / snsAsp2["std_dev1"]
snsAsp3 = snsAsp2[snsAsp2["std_dev_away1"]<2]


# Compute the summary statistics
summary_stats2 = snsAsp3.groupby('445 Year').agg(
    min_price2=('asp', 'min'),
    max_price2=('asp', 'max'),
    median_price2=('asp', 'median'),
    first_quartile2=('asp', lambda x: x.quantile(0.25)),
    third_quartile2=('asp', lambda x: x.quantile(0.75)),
    mean_price2=('asp', 'mean'),
    std_dev2=('asp', np.std)
).reset_index()

pp = data6[(data6['CUST_CLASS_3']=='STANDARD') 
             & (data6['DIVISION']=='PRODUCT') 
             & (data6['STOCK_OR_NOT'] == 'USER_ORDER')]

## Exclude the Items that have more than Sensor in them
snsEx = ['10132916',
'08225013',
'10137113',
'10137115']

senpp = pp[(pp['CATEGORY']=='SENSOR') & (pp['Net Amount USD']>0) & (pp['Net QTY']>0)]
senpp = senpp[~senpp['Product Number'].isin(snsEx)]
senpp['asp'] = senpp['Net Amount USD']/senpp['Net QTY']

senpp = senpp.merge(summary_stats2, on="445 Year", how="left")
senpp["std_dev_away2"] = (senpp["asp"] - senpp["mean_price2"]) / senpp["std_dev2"]

nonsns = ['3D_CBCT',
'2D_PAN',
'3D_I_CAT',
'NOMAD_PRO_2',
'3D_CBCT_CEPH',
'2D_CEPH']

pp1 = seg1.copy()

pp1 = pp1[pp1['PRODUCT'].isin(nonsns)]


# Compute the summary statistics
pp1 = pp1[(pp1['Net Amount USD']>0) & (pp1['Net QTY']>0)]
pp1['asp'] = pp1['Net Amount USD']/pp1['Net QTY']


summary_stats_all = pp1.groupby(['445 Year','PRODUCT']).agg(
    min_price=('asp', 'min'),
    max_price=('asp', 'max'),
    median_price=('asp', 'median'),
    first_quartile=('asp', lambda x: x.quantile(0.25)),
    third_quartile=('asp', lambda x: x.quantile(0.75)),
    mean_price=('asp', 'mean'),
    std_dev=('asp', np.std)
).reset_index()

pp1 = pp1.merge(summary_stats_all, on=["445 Year","PRODUCT"], how="left")
pp1["std_dev_away"] = (pp1["asp"] - pp1["mean_price"]) / pp1["std_dev"]


pp2 = pp1[(pp1['std_dev_away']>-3) & (pp1['std_dev_away']<3)]
pp2 = pp2[pp2['BILL_MDM_ACCT_OFFICE_NAME']!='RENEW DIGITAL']
pp2['std_dev_away_Rnd'] = pp2['std_dev_away'].round(1)

summary_stats_all2 = pp2.groupby(['445 Year','PRODUCT']).agg(
    min_price2=('asp', 'min'),
    max_price2=('asp', 'max'),
    median_price2=('asp', 'median'),
    first_quartile2=('asp', lambda x: x.quantile(0.25)),
    third_quartile2=('asp', lambda x: x.quantile(0.75)),
    mean_price2=('asp', 'mean'),
    std_dev2=('asp', np.std)
).reset_index()

summary_stats_all2 = summary_stats_all2.sort_values(by=['PRODUCT','445 Year'])

snsixs = summary_stats2.copy()
snsixs['PRODUCT'] ='SENSOR_IXS'
snstin = summary_stats2.copy()
snstin['PRODUCT'] ='SENSOR_TITANIUM'

snsttl = pd.concat([snstin,snsixs])
stats = pd.concat([summary_stats_all2,snsttl])


seg1['asp'] = seg1['Net Amount USD']/seg1['Net QTY']
seg1_v1 = seg1.merge(stats, on=["445 Year","PRODUCT"], how="left")
seg1_v1["std_dev_away2"] = (seg1_v1["asp"] - seg1_v1["mean_price2"]) / seg1_v1["std_dev2"]
seg1_v1["std_rnd"] = seg1_v1["std_dev_away2"].round(1)
seg1_v1["std_rnd"] = seg1_v1["std_rnd"].fillna(-10)

nonpp = {
'2019_2D_CEPH' : -0.5,
'2019_2D_PAN' : -0.5,
'2019_3D_CBCT' : -0.5,
'2019_3D_CBCT_CEPH' : -0.5,
'2019_3D_I_CAT' : -0.5,
'2019_CAMERA_DEXCAM4' : -0.5,
'2019_CR_READER_OPTIME' : -0.5,
'2019_OTHER_CRANEX_3D' : -0.5,
'2019_OTHER_OTHER' : -0.5,
'2019_SENSOR_ERGO' : 0.5,
'2019_SENSOR_IXS' : 0.5,
'2019_SENSOR_TITANIUM' : 0.5,
'2020_2D_CEPH' : -0.5,
'2020_2D_PAN' : -0.5,
'2020_3D_CBCT' : -0.5,
'2020_3D_CBCT_CEPH' : -0.5,
'2020_3D_I_CAT' : -0.5,
'2020_3D_RENEW_DIG' : -0.5,
'2020_CAMERA_DEXCAM4' : -0.5,
'2020_CR_READER_OPTIME' : -0.5,
'2020_NOMAD_PRO_2' : -0.5,
'2020_OTHER_OTHER' : -0.5,
'2020_SENSOR_ERGO' : 1,
'2020_SENSOR_IXS' : 1,
'2020_SENSOR_TITANIUM' : 1,
'2021_2D_CEPH' : -0.5,
'2021_2D_PAN' : -0.5,
'2021_3D_CBCT' : -0.5,
'2021_3D_CBCT_CEPH' : -0.5,
'2021_3D_I_CAT' : -0.5,
'2021_CAMERA_DEXCAM4' : -0.5,
'2021_CR_READER_OPTIME' : -0.5,
'2021_NOMAD_PRO_2' : -0.5,
'2021_OTHER_OTHER' : -0.5,
'2021_SENSOR_ERGO' : 2.5,
'2021_SENSOR_IXS' : 2.5,
'2021_SENSOR_TITANIUM' : 2.5,
'2022_2D_CEPH' : -0.25,
'2022_2D_PAN' : -0.25,
'2022_3D_CBCT' : -0.25,
'2022_3D_CBCT_CEPH' : -0.25,
'2022_3D_I_CAT' : -0.25,
'2022_CAMERA_DEXCAM4' : -0.25,
'2022_CR_READER_OPTIME' : -0.25,
'2022_NOMAD_PRO_2' : -0.25,
'2022_OTHER_OTHER' : -0.25,
'2022_SENSOR_ERGO' : 2.4,
'2022_SENSOR_IXS' : 2.4,
'2022_SENSOR_TITANIUM' : 2.4,
'2023_2D_CEPH' : -0.25,
'2023_2D_PAN' : -0.25,
'2023_3D_CBCT' : -0.25,
'2023_3D_CBCT_CEPH' : -0.25,
'2023_3D_I_CAT' : -0.25,
'2023_CAMERA_DEXCAM4' : -0.25,
'2023_CR_READER_OPTIME' : -0.25,
'2023_NOMAD_PRO_2' : -0.25,
'2023_SENSOR_IXS' : 2,
'2023_SENSOR_TITANIUM' : 2
}

nonpp_class = pd.DataFrame.from_dict(nonpp,orient='index')
nonpp_class = nonpp_class.reset_index()
nonpp_class = nonpp_class.rename(columns={'index': 'ref_key',0 : 'std_dev_ref'})
seg1_v1['ref_key'] = seg1_v1['445 Year'].astype(str)+"_"+seg1_v1['PRODUCT']
seg1_v2 = pd.merge(seg1_v1,nonpp_class, on = 'ref_key',how='left')
seg1_v2['pp_npp'] = np.where(seg1_v2['std_rnd'] <= seg1_v2['std_dev_ref'],'NON_PP','PP')

seg1_v3 = seg1_v2[seg1_v2['pp_npp']=='PP']

# Compute the summary statistics
seg1_v4 = seg1_v3[(seg1_v3['Net Amount USD']>0) & (seg1_v3['Net QTY']>0)]

ppStats = seg1_v4.groupby(['445 Year','PRODUCT']).agg(
    min_price=('asp', 'min'),
    max_price=('asp', 'max'),
    median_price=('asp', 'median'),
    first_quartile=('asp', lambda x: x.quantile(0.25)),
    third_quartile=('asp', lambda x: x.quantile(0.75)),
    mean_price=('asp', 'mean'),
    std_dev=('asp', np.std)
).reset_index()

ppStats = ppStats.sort_values(by=['PRODUCT','445 Year'])
ppStats[ppStats['445 Year']=='2023'][['PRODUCT','mean_price']].round()


seg1_v5 = seg1_v3.copy()

seg1_v5 = seg1_v5.merge(ppStats, on=["445 Year","PRODUCT"], how="left")
seg1_v5["std_dev_away"] = (seg1_v5["asp"] - seg1_v5["mean_price"]) / seg1_v5["std_dev"]

seg1_v5['std_rnd'] = seg1_v5['std_dev_away'].round(1)
ordPrd = seg1_v4.groupby(['445 Year','445 Quarter','445 Month','445 Week','REGION','TERR','3DIGIT','SALES_ORDER_NUMBER']).agg(
    prd_grp = ('PRODUCT',lambda x: ' | '.join(set(x))),
    sumOrd = ('Net Amount USD','sum')
    ).reset_index()
ordPrd['445 Quarter'] = ordPrd['445 Quarter'].astype(int)

ordPrd = ordPrd.sort_values(by=['REGION',
                                'prd_grp',
                                '445 Year',
                                '445 Quarter',
                                '445 Month',
                                '445 Week']
                           )


ordStats = ordPrd.groupby(['445 Year','445 Quarter','REGION','prd_grp']).agg(
    min_ord=('sumOrd', 'min'),
    max_ord=('sumOrd', 'max'),
    median_ord=('sumOrd', 'median'),
    first_quartile_ord=('sumOrd', lambda x: x.quantile(0.25)),
    third_quartile_ord=('sumOrd', lambda x: x.quantile(0.75)),
    mean_ord=('sumOrd', 'mean'),
    std_dev_ord=('sumOrd', np.std)
).round(0).reset_index()

ordStats = ordStats.sort_values(by=['REGION','445 Year','445 Quarter'])

ordPrd2 = ordPrd.merge(ordStats, on=['445 Year','445 Quarter','REGION','prd_grp'], how="left")
ordPrd2["std_dev_away_ord"] = (ordPrd2["sumOrd"] - ordPrd2["mean_ord"]) / ordPrd2["std_dev_ord"]
ordPrd2 = ordPrd2.sort_values(by=['REGION','445 Year','445 Month','445 Week'])


#Greater than 2 Std Deviations

lower = -2
upper = 2
ordPrd2['nrm_out'] = ordPrd2['std_dev_away_ord'].apply(lambda x : 'normal' if lower <= x <= upper else 'outlier')
ordPrd2['nrm_out'].value_counts(dropna=False)
prdGrp = ['3D_CBCT',
'3D_I_CAT',
'SENSOR_TITANIUM',
'2D_PAN',
'3D_CBCT_CEPH',
'SENSOR_IXS',
'NOMAD_PRO_2',
'2D_CEPH']
ordPrd3 = ordPrd2[ordPrd2['prd_grp'].isin(prdGrp)]
ordPrd3 = ordPrd3.fillna(0)
ordPrd_nml = ordPrd3[ordPrd3['nrm_out']=='normal']
ordPrd_out = ordPrd3[ordPrd3['nrm_out']=='outlier']
ordPrd_out['realloc_months'] = ordPrd_out['sumOrd']/((ordPrd_out['std_dev_ord']*upper)+ordPrd_out['mean_ord'])



ordPrd_out['realloc_months'] = np.ceil(ordPrd_out['realloc_months'])
ordPrd_out['realloc_months'].value_counts(dropna=False)

# Expand rows based on realloc
df_expanded = ordPrd_out.loc[ordPrd_out.index.repeat(ordPrd_out['realloc_months'])].reset_index(drop=True)
df_expanded['445 Year'] = df_expanded['445 Year'].astype(int)
df_expanded['445 Month'] = df_expanded['445 Month'].astype('float64')
df_expanded['445 Week'] = df_expanded['445 Week'].astype(int)

# Calculate the incremented month and year
moConv={1:4,
2:4,
3:5,
4:4,
5:4,
6:5,
7:4,
8:4,
9:5,
10:4,
11:4,
12:5}

df_expanded['adjust_Year'] = df_expanded['445 Year'] + (df_expanded['445 Month'] - 1 + df_expanded.groupby(['SALES_ORDER_NUMBER','REGION','prd_grp', '445 Month']).cumcount()) // 12
df_expanded['adjusted_month'] = (df_expanded['445 Month'] + df_expanded.groupby(['SALES_ORDER_NUMBER','REGION','prd_grp', '445 Month']).cumcount() - 1) % 12 + 1

df_expanded['mo_conv'] = df_expanded['adjusted_month'].map(moConv)

df_expanded['adj_weeks'] = df_expanded['445 Week']+4
df_expanded['adj_weeks2'] = np.where(
                                    df_expanded['adjusted_month']==df_expanded['445 Month'],
                                    df_expanded['445 Week'],
                                    np.where(
                                        (df_expanded['445 Week']+df_expanded['mo_conv']) <= 52,
                                        (df_expanded['445 Week']+df_expanded['mo_conv']),
                                        (df_expanded['445 Week']+df_expanded['mo_conv'])-52)
                                    )

# Calculate amt_realloc
df_expanded['amt_realloc'] = df_expanded['sumOrd'] / df_expanded['realloc_months']


dfcol = [
    'adjust_Year','adjusted_month','adj_weeks2', 'REGION', 'TERR',
       '3DIGIT', 'SALES_ORDER_NUMBER','prd_grp','sumOrd',
    'min_ord', 'max_ord','median_ord', 'first_quartile_ord', 'third_quartile_ord', 'mean_ord',
    'std_dev_ord', 'std_dev_away_ord','amt_realloc'
    ]
smoutl = df_expanded[dfcol]
smcln ={
    'adjust_Year' : '445 Year',
    'adjusted_month' : '445 Month',
    'adj_weeks2' : '445 Week',
}
smoutl = smoutl.rename(columns=smcln)

ordPrd_nml['amt_realloc'] = ordPrd_nml['sumOrd'].copy()
segsm = pd.concat([ordPrd_nml,smoutl])



col3 = ['445 Year', '445 Month','445 Week', 'REGION', 'TERR','3DIGIT', 'prd_grp']
segsm1 = segsm.groupby(col3)['amt_realloc'].sum().reset_index()
segsm1['TERR'] = segsm1['TERR'].str.replace(' - ','_')
segsm1['REGION'] = segsm1['REGION'].str.replace(' ','_')
segsm1['REG_TERR'] = segsm1['REGION']+'_'+segsm1['TERR']
segsm1['445 Year'] = segsm1['445 Year'].astype(int)
segsm1['445 Year'] = segsm1['445 Year'].astype(str)
segsm1['445 Week'] = segsm1['445 Week'].astype(str)
segsm2 = pd.merge(segsm1,pop3,on=['445 Year','REG_TERR'],how='left')
segsm2 = segsm2.rename(columns={'value':'pop_change','amt_realloc':'sales','prd_grp':'PRODUCT'})

col = ['445 Year', '445 Week' ,'REG_TERR','PRODUCT','pop_change']
segsm3 = segsm2.groupby(col)['sales'].sum().reset_index()

all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = segsm3['REG_TERR'].unique()
all_products = segsm3['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year', '445 Week', 'REG_TERR', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, segsm3, on=['445 Year', '445 Week', 'REG_TERR','PRODUCT'], how='left')
merged_df = merged_df.drop(['pop_change'], axis=1)
merged_df = pd.merge(merged_df,pop3,on=['445 Year','REG_TERR'],how='left')
merged_df = merged_df.rename(columns={'value':'pop_change'})

# Fill NaNs with 0
segsm2 = pd.merge(segsm1,pop3,on=['445 Year','REG_TERR'],how='left')
merged_df['sales'].fillna(0, inplace=True)
merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR','PRODUCT','pop_change']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# 1. Preprocessing

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT', 'REG_TERR']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)

regMap = {'CANADA_CAN_1' : 'CANADA',
'CANADA_CAN_2' : 'CANADA',
'CANADA_CAN_3' : 'CANADA',
'CANADA_CAN_4' : 'CANADA',
'NORTH_CENTRAL_NC_1' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_2' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_3' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_4' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_5' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_6' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_7' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_8' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_9' : 'NORTH CENTRAL',
'NORTHEAST_NE_1' : 'NORTHEAST',
'NORTHEAST_NE_2' : 'NORTHEAST',
'NORTHEAST_NE_3' : 'NORTHEAST',
'NORTHEAST_NE_5' : 'NORTHEAST',
'NORTHEAST_NE_6' : 'NORTHEAST',
'NORTHEAST_NE_7' : 'NORTHEAST',
'NORTHEAST_NE_8' : 'NORTHEAST',
'NORTHEAST_NE_9' : 'NORTHEAST',
'SOUTH_CENTRAL_SC_1' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_10' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_2' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_3' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_6' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_7' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_8' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_9' : 'SOUTH CENTRAL',
'SOUTHEAST_SE_1' : 'SOUTHEAST',
'SOUTHEAST_SE_2' : 'SOUTHEAST',
'SOUTHEAST_SE_3' : 'SOUTHEAST',
'SOUTHEAST_SE_4' : 'SOUTHEAST',
'SOUTHEAST_SE_5' : 'SOUTHEAST',
'SOUTHEAST_SE_6' : 'SOUTHEAST',
'SOUTHEAST_SE_7' : 'SOUTHEAST',
'SOUTHEAST_SE_8' : 'SOUTHEAST',
'SOUTHEAST_SE_9' : 'SOUTHEAST',
'WEST_W_1' : 'WEST',
'WEST_W_10' : 'WEST',
'WEST_W_11' : 'WEST',
'WEST_W_2' : 'WEST',
'WEST_W_3' : 'WEST',
'WEST_W_4' : 'WEST',
'WEST_W_5' : 'WEST',
'WEST_W_7' : 'WEST',
'WEST_W_9' : 'WEST'
}

test_df['REGION'] = test_df['REG_TERR'].map(regMap)
test_df['cal_ref'] = test_df['REGION']+'_'+test_df['month'].astype(str)

calibration_factor = {
'CANADA_1' : 0.99,
'CANADA_2' : 0.56,
'CANADA_3' : 1.3,
'CANADA_4' : 0.74,
'CANADA_5' : 1.37,
'CANADA_6' : 1.44,
'CANADA_7' : 0.74,
'CANADA_8' : 1,
'CANADA_9' : 1.45,
'CANADA_10' : 2,
'CANADA_11' : 2.75,
'CANADA_12' : 2.75,
'NORTH CENTRAL_1' : 0.31,
'NORTH CENTRAL_2' : 0.67,
'NORTH CENTRAL_3' : 1.09,
'NORTH CENTRAL_4' : 0.7,
'NORTH CENTRAL_5' : 1.35,
'NORTH CENTRAL_6' : 1.1,
'NORTH CENTRAL_7' : 0.69,
'NORTH CENTRAL_8' : 1.33,
'NORTH CENTRAL_9' : 1.35,
'NORTH CENTRAL_10' : 1.2,
'NORTH CENTRAL_11' : 1.3,
'NORTH CENTRAL_12' : 1.4,
'NORTHEAST_1' : 0.47,
'NORTHEAST_2' : 0.65,
'NORTHEAST_3' : 1.02,
'NORTHEAST_4' : 0.62,
'NORTHEAST_5' : 1.73,
'NORTHEAST_6' : 1.18,
'NORTHEAST_7' : 0.96,
'NORTHEAST_8' : 1.1,
'NORTHEAST_9' : 1.30,
'NORTHEAST_10' : 1,
'NORTHEAST_11' : 1.25,
'NORTHEAST_12' : 1.75,
'SOUTH CENTRAL_1' : 0.8,
'SOUTH CENTRAL_2' : 0.61,
'SOUTH CENTRAL_3' : 1.02,
'SOUTH CENTRAL_4' : 1.48,
'SOUTH CENTRAL_5' : 2.63,
'SOUTH CENTRAL_6' : 1.6,
'SOUTH CENTRAL_7' : 1.1,
'SOUTH CENTRAL_8' : 1.96,
'SOUTH CENTRAL_9' : 1.30,
'SOUTH CENTRAL_10' : 1.5,
'SOUTH CENTRAL_11' : 1.5,
'SOUTH CENTRAL_12' : 1.75,
'SOUTHEAST_1' : 1.28,
'SOUTHEAST_2' : 1.15,
'SOUTHEAST_3' : 1.43,
'SOUTHEAST_4' : 1.24,
'SOUTHEAST_5' : 2.29,
'SOUTHEAST_6' : 1.24,
'SOUTHEAST_7' : 1.5,
'SOUTHEAST_8' : 1.7,
'SOUTHEAST_9' : 1.55,
'SOUTHEAST_10' : 3,
'SOUTHEAST_11' : 3,
'SOUTHEAST_12' : 3,
'WEST_1' : 0.9,
'WEST_2' : 0.89,
'WEST_3' : 1.74,
'WEST_4' : 1,
'WEST_5' : 2.2,
'WEST_6' : 1.53,
'WEST_7' : 0.79,
'WEST_8' : 1.66,
'WEST_9' : 1.35,
'WEST_10' : 1.5,
'WEST_11' : 1.55,
'WEST_12' : 1.75
}

test_df['calib_fct'] = test_df['cal_ref'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']
test_df = test_df.drop(['cal_ref','calib_fct'],axis=1)
tsmPred = test_df.copy()
tsmPred = tsmPred.drop(['pop_change'],axis=1)

sla = data7[data7['DIVISION']=='SLA']

slaSeg = sla.copy()

slaSeg['TERR'] = slaSeg['TERR'].str.replace(' - ','_')
slaSeg['REGION'] = slaSeg['REGION'].str.replace(' ','_')

slaSeg['REG_TERR'] = slaSeg['REGION']+'_'+slaSeg['TERR']
slaSeg['445 Year'] = slaSeg['445 Year'].astype(int)
slaSeg['445 Year'] = slaSeg['445 Year'].astype(str)
slaSeg['445 Week'] = slaSeg['445 Week'].astype(str)

slaSeg = slaSeg.rename(columns={'Net Amount USD':'sales'})

col = ['445 Year', '445 Week' ,'REG_TERR','PRODUCT']
slaSeg2 = slaSeg.groupby(col)['sales'].sum().reset_index()

slaSeg2['PRODUCT'].value_counts(dropna=False)
excl = ['SENSOR_FEES','OTHER_FEES','3D_OTHER','SOFTWARE_SOFTWARE','2D_UPLIFT']
slaSeg3 = slaSeg2[~slaSeg2['PRODUCT'].isin(excl)]
slaSeg3['PRODUCT'].value_counts(dropna=False)

all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = slaSeg3['REG_TERR'].unique()
all_products = slaSeg3['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year',  '445 Week', 'REG_TERR', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, slaSeg3, on=['445 Year', '445 Week', 'REG_TERR','PRODUCT'], how='left')
merged_df['sales'].fillna(0, inplace=True)

merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR','PRODUCT']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT', 'REG_TERR']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)

regMap = {'CANADA_CAN_1' : 'CANADA',
'CANADA_CAN_2' : 'CANADA',
'CANADA_CAN_3' : 'CANADA',
'CANADA_CAN_4' : 'CANADA',
'NORTH_CENTRAL_NC_1' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_2' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_3' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_4' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_5' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_6' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_7' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_8' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_9' : 'NORTH CENTRAL',
'NORTHEAST_NE_1' : 'NORTHEAST',
'NORTHEAST_NE_2' : 'NORTHEAST',
'NORTHEAST_NE_3' : 'NORTHEAST',
'NORTHEAST_NE_5' : 'NORTHEAST',
'NORTHEAST_NE_6' : 'NORTHEAST',
'NORTHEAST_NE_7' : 'NORTHEAST',
'NORTHEAST_NE_8' : 'NORTHEAST',
'NORTHEAST_NE_9' : 'NORTHEAST',
'SOUTH_CENTRAL_SC_1' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_10' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_2' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_3' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_6' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_7' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_8' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_9' : 'SOUTH CENTRAL',
'SOUTHEAST_SE_1' : 'SOUTHEAST',
'SOUTHEAST_SE_2' : 'SOUTHEAST',
'SOUTHEAST_SE_3' : 'SOUTHEAST',
'SOUTHEAST_SE_4' : 'SOUTHEAST',
'SOUTHEAST_SE_5' : 'SOUTHEAST',
'SOUTHEAST_SE_6' : 'SOUTHEAST',
'SOUTHEAST_SE_7' : 'SOUTHEAST',
'SOUTHEAST_SE_8' : 'SOUTHEAST',
'SOUTHEAST_SE_9' : 'SOUTHEAST',
'WEST_W_1' : 'WEST',
'WEST_W_10' : 'WEST',
'WEST_W_11' : 'WEST',
'WEST_W_2' : 'WEST',
'WEST_W_3' : 'WEST',
'WEST_W_4' : 'WEST',
'WEST_W_5' : 'WEST',
'WEST_W_7' : 'WEST',
'WEST_W_9' : 'WEST'
}

test_df['REGION'] = test_df['REG_TERR'].map(regMap)

calibration_factor = {1:0.85 ,
                      2:1 ,
                      3:1.45, 
                      4:0.95,
                      5:1.1, 
                      6:1.25 ,
                      7:0.9 ,
                      8:0.95 ,
                      9:1.25 ,
                      10:0.85 ,
                      11:0.95 ,
                      12:1.25}

test_df['calib_fct'] = test_df['month'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']

test_df = test_df.drop(['calib_fct'],axis=1)

slaPred = test_df.copy()

stck = data7[data7['SEG_GRP']=='STCK']

stck['TERR'] = stck['TERR'].str.replace(' - ','_')
stck['REGION'] = stck['REGION'].str.replace(' ','_')

stck['REG_TERR'] = stck['REGION']+'_'+stck['TERR']
stck['445 Year'] = stck['445 Year'].astype(int)
stck['445 Year'] = stck['445 Year'].astype(str)
stck['445 Week'] = stck['445 Week'].astype(str)

stck = stck.rename(columns={'Net Amount USD':'sales'})

col = ['445 Year', '445 Week' ,'REG_TERR','PRODUCT']
stck2 = stck.groupby(col)['sales'].sum().reset_index()


all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = stck2['REG_TERR'].unique()
all_products = stck2['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year',  '445 Week', 'REG_TERR', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, stck2, on=['445 Year', '445 Week', 'REG_TERR','PRODUCT'], how='left')
merged_df['sales'].fillna(0, inplace=True)

merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR','PRODUCT']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT', 'REG_TERR']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)

regMap = {'CANADA_CAN_1' : 'CANADA',
'CANADA_CAN_2' : 'CANADA',
'CANADA_CAN_3' : 'CANADA',
'CANADA_CAN_4' : 'CANADA',
'NORTH_CENTRAL_NC_1' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_2' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_3' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_4' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_5' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_6' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_7' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_8' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_9' : 'NORTH CENTRAL',
'NORTHEAST_NE_1' : 'NORTHEAST',
'NORTHEAST_NE_2' : 'NORTHEAST',
'NORTHEAST_NE_3' : 'NORTHEAST',
'NORTHEAST_NE_5' : 'NORTHEAST',
'NORTHEAST_NE_6' : 'NORTHEAST',
'NORTHEAST_NE_7' : 'NORTHEAST',
'NORTHEAST_NE_8' : 'NORTHEAST',
'NORTHEAST_NE_9' : 'NORTHEAST',
'SOUTH_CENTRAL_SC_1' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_10' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_2' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_3' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_6' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_7' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_8' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_9' : 'SOUTH CENTRAL',
'SOUTHEAST_SE_1' : 'SOUTHEAST',
'SOUTHEAST_SE_2' : 'SOUTHEAST',
'SOUTHEAST_SE_3' : 'SOUTHEAST',
'SOUTHEAST_SE_4' : 'SOUTHEAST',
'SOUTHEAST_SE_5' : 'SOUTHEAST',
'SOUTHEAST_SE_6' : 'SOUTHEAST',
'SOUTHEAST_SE_7' : 'SOUTHEAST',
'SOUTHEAST_SE_8' : 'SOUTHEAST',
'SOUTHEAST_SE_9' : 'SOUTHEAST',
'WEST_W_1' : 'WEST',
'WEST_W_10' : 'WEST',
'WEST_W_11' : 'WEST',
'WEST_W_2' : 'WEST',
'WEST_W_3' : 'WEST',
'WEST_W_4' : 'WEST',
'WEST_W_5' : 'WEST',
'WEST_W_7' : 'WEST',
'WEST_W_9' : 'WEST'
}

test_df['REGION'] = test_df['REG_TERR'].map(regMap)

calibration_factor = {1:.65 ,
                      2: .65,
                      3:.65, 
                      4:.65,
                      5:.65, 
                      6:.65 ,
                      7:.6 ,
                      8:.6 ,
                      9:.3 ,
                      10:.35 ,
                      11:.35 ,
                      12:.35}

test_df['calib_fct'] = test_df['month'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']

test_df = test_df.drop(['calib_fct'],axis=1)

stckPred = test_df.copy()

prts = data7[data7['SEG_GRP']=='PRTS']

prts['TERR'] = prts['TERR'].str.replace(' - ','_')
prts['REGION'] = prts['REGION'].str.replace(' ','_')

prts['REG_TERR'] = prts['REGION']+'_'+prts['TERR']
prts['445 Year'] = prts['445 Year'].astype(int)
prts['445 Year'] = prts['445 Year'].astype(str)
prts['445 Week'] = prts['445 Week'].astype(str)

prts = prts.rename(columns={'Net Amount USD':'sales'})

col = ['445 Year', '445 Week' ,'REG_TERR','PRODUCT']
prts2 = prts.groupby(col)['sales'].sum().reset_index()


all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = prts2['REG_TERR'].unique()
all_products = prts2['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year',  '445 Week', 'REG_TERR', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, prts2, on=['445 Year', '445 Week', 'REG_TERR','PRODUCT'], how='left')
merged_df['sales'].fillna(0, inplace=True)

merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR','PRODUCT']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT', 'REG_TERR']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)

regMap = {'CANADA_CAN_1' : 'CANADA',
'CANADA_CAN_2' : 'CANADA',
'CANADA_CAN_3' : 'CANADA',
'CANADA_CAN_4' : 'CANADA',
'NORTH_CENTRAL_NC_1' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_2' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_3' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_4' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_5' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_6' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_7' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_8' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_9' : 'NORTH CENTRAL',
'NORTHEAST_NE_1' : 'NORTHEAST',
'NORTHEAST_NE_2' : 'NORTHEAST',
'NORTHEAST_NE_3' : 'NORTHEAST',
'NORTHEAST_NE_5' : 'NORTHEAST',
'NORTHEAST_NE_6' : 'NORTHEAST',
'NORTHEAST_NE_7' : 'NORTHEAST',
'NORTHEAST_NE_8' : 'NORTHEAST',
'NORTHEAST_NE_9' : 'NORTHEAST',
'SOUTH_CENTRAL_SC_1' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_10' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_2' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_3' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_6' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_7' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_8' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_9' : 'SOUTH CENTRAL',
'SOUTHEAST_SE_1' : 'SOUTHEAST',
'SOUTHEAST_SE_2' : 'SOUTHEAST',
'SOUTHEAST_SE_3' : 'SOUTHEAST',
'SOUTHEAST_SE_4' : 'SOUTHEAST',
'SOUTHEAST_SE_5' : 'SOUTHEAST',
'SOUTHEAST_SE_6' : 'SOUTHEAST',
'SOUTHEAST_SE_7' : 'SOUTHEAST',
'SOUTHEAST_SE_8' : 'SOUTHEAST',
'SOUTHEAST_SE_9' : 'SOUTHEAST',
'WEST_W_1' : 'WEST',
'WEST_W_10' : 'WEST',
'WEST_W_11' : 'WEST',
'WEST_W_2' : 'WEST',
'WEST_W_3' : 'WEST',
'WEST_W_4' : 'WEST',
'WEST_W_5' : 'WEST',
'WEST_W_7' : 'WEST',
'WEST_W_9' : 'WEST'
}

test_df['REGION'] = test_df['REG_TERR'].map(regMap)

calibration_factor = {1:.85 ,
                      2: .85,
                      3:.85, 
                      4:.85,
                      5:.85, 
                      6:.85 ,
                      7:.85 ,
                      8:.85 ,
                      9:.85 ,
                      10:.85 ,
                      11:.85 ,
                      12:.85}

test_df['calib_fct'] = test_df['month'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']

test_df = test_df.drop(['calib_fct'],axis=1)

inst = data7[data7['SEG_GRP']=='INST']

inst['TERR'] = inst['TERR'].str.replace(' - ','_')
inst['REGION'] = inst['REGION'].str.replace(' ','_')


inst['REG_TERR'] = inst['REGION']+'_'+inst['TERR']
inst['REG_TERR_CUST'] = inst['REGION']+'|'+inst['CUST_SEGMENT']
inst['445 Year'] = inst['445 Year'].astype(int)
inst['445 Year'] = inst['445 Year'].astype(str)
inst['445 Week'] = inst['445 Week'].astype(str)

inst = inst.rename(columns={'Net Amount USD':'sales'})

col = ['445 Year', '445 Week' ,'REG_TERR_CUST','PRODUCT']
inst2 = inst.groupby(col)['sales'].sum().reset_index()

all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = inst2['REG_TERR_CUST'].unique()
all_products = inst2['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year',  '445 Week','REG_TERR_CUST', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, inst2, on=['445 Year', '445 Week', 'REG_TERR_CUST','PRODUCT'], how='left')
merged_df['sales'].fillna(0, inplace=True)

merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR_CUST','PRODUCT']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR_CUST']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR_CUST', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT','REG_TERR_CUST']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR_CUST', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)

test_df[["REGION","CUST_SEGMENT"]]= test_df["REG_TERR_CUST"].str.split("|",n=1,expand=True)

calibration_factor = {1:1 ,
                      2: 1,
                      3:1, 
                      4:1,
                      5:1, 
                      6:1 ,
                      7:1 ,
                      8:1 ,
                      9:1 ,
                      10:1 ,
                      11:1 ,
                      12:1}

test_df['calib_fct'] = test_df['month'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']

test_df = test_df.drop(['calib_fct'],axis=1)
instPred = test_df.copy()
edso = data7[data7['SEG_GRP']=='EDSO']

edso['TERR'] = edso['TERR'].str.replace(' - ','_')
edso['REGION'] = edso['REGION'].str.replace(' ','_')

edso['REG_TERR'] = edso['REGION']+'_'+edso['TERR']
edso['445 Year'] = edso['445 Year'].astype(int)
edso['445 Year'] = edso['445 Year'].astype(str)
edso['445 Week'] = edso['445 Week'].astype(str)

edso = edso.rename(columns={'Net Amount USD':'sales'})

col = ['445 Year', '445 Week' ,'REG_TERR','PRODUCT']
edso2 = edso.groupby(col)['sales'].sum().reset_index()

all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = edso2['REG_TERR'].unique()
all_products = edso2['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year',  '445 Week', 'REG_TERR', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, edso2, on=['445 Year', '445 Week', 'REG_TERR','PRODUCT'], how='left')
merged_df['sales'].fillna(0, inplace=True)

merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR','PRODUCT']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT','REG_TERR']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)

regMap = {'CANADA_CAN_1' : 'CANADA',
'CANADA_CAN_2' : 'CANADA',
'CANADA_CAN_3' : 'CANADA',
'CANADA_CAN_4' : 'CANADA',
'NORTH_CENTRAL_NC_1' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_2' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_3' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_4' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_5' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_6' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_7' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_8' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_9' : 'NORTH CENTRAL',
'NORTHEAST_NE_1' : 'NORTHEAST',
'NORTHEAST_NE_2' : 'NORTHEAST',
'NORTHEAST_NE_3' : 'NORTHEAST',
'NORTHEAST_NE_5' : 'NORTHEAST',
'NORTHEAST_NE_6' : 'NORTHEAST',
'NORTHEAST_NE_7' : 'NORTHEAST',
'NORTHEAST_NE_8' : 'NORTHEAST',
'NORTHEAST_NE_9' : 'NORTHEAST',
'SOUTH_CENTRAL_SC_1' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_10' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_2' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_3' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_6' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_7' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_8' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_9' : 'SOUTH CENTRAL',
'SOUTHEAST_SE_1' : 'SOUTHEAST',
'SOUTHEAST_SE_2' : 'SOUTHEAST',
'SOUTHEAST_SE_3' : 'SOUTHEAST',
'SOUTHEAST_SE_4' : 'SOUTHEAST',
'SOUTHEAST_SE_5' : 'SOUTHEAST',
'SOUTHEAST_SE_6' : 'SOUTHEAST',
'SOUTHEAST_SE_7' : 'SOUTHEAST',
'SOUTHEAST_SE_8' : 'SOUTHEAST',
'SOUTHEAST_SE_9' : 'SOUTHEAST',
'WEST_W_1' : 'WEST',
'WEST_W_10' : 'WEST',
'WEST_W_11' : 'WEST',
'WEST_W_2' : 'WEST',
'WEST_W_3' : 'WEST',
'WEST_W_4' : 'WEST',
'WEST_W_5' : 'WEST',
'WEST_W_7' : 'WEST',
'WEST_W_9' : 'WEST'
}

test_df['REGION'] = test_df['REG_TERR'].map(regMap)
calibration_factor = {1:1 ,
                      2: 1,
                      3:1, 
                      4:1,
                      5:1, 
                      6:1 ,
                      7:1 ,
                      8:1 ,
                      9:1 ,
                      10:1 ,
                      11:1 ,
                      12:1}

test_df['calib_fct'] = test_df['month'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']

test_df = test_df.drop(['calib_fct'],axis=1)
edsoPred = test_df.copy()

ncom = data7[data7['SEG_GRP']=='NCOM']

ncom['TERR'] = ncom['TERR'].str.replace(' - ','_')
ncom['REGION'] = ncom['REGION'].str.replace(' ','_')

ncom['REG_TERR'] = ncom['REGION']+'_'+ncom['TERR']
ncom['445 Year'] = ncom['445 Year'].astype(int)
ncom['445 Year'] = ncom['445 Year'].astype(str)
ncom['445 Week'] = ncom['445 Week'].astype(str)

ncom = ncom.rename(columns={'Net Amount USD':'sales'})

col = ['445 Year', '445 Week' ,'REG_TERR','PRODUCT']
ncom2 = ncom.groupby(col)['sales'].sum().reset_index()

all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = ncom2['REG_TERR'].unique()
all_products = ncom2['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year',  '445 Week', 'REG_TERR', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, ncom2, on=['445 Year', '445 Week', 'REG_TERR','PRODUCT'], how='left')
merged_df['sales'].fillna(0, inplace=True)

merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR','PRODUCT']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT','REG_TERR']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)

regMap = {'CANADA_CAN_1' : 'CANADA',
'CANADA_CAN_2' : 'CANADA',
'CANADA_CAN_3' : 'CANADA',
'CANADA_CAN_4' : 'CANADA',
'NORTH_CENTRAL_NC_1' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_2' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_3' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_4' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_5' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_6' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_7' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_8' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_9' : 'NORTH CENTRAL',
'NORTHEAST_NE_1' : 'NORTHEAST',
'NORTHEAST_NE_2' : 'NORTHEAST',
'NORTHEAST_NE_3' : 'NORTHEAST',
'NORTHEAST_NE_5' : 'NORTHEAST',
'NORTHEAST_NE_6' : 'NORTHEAST',
'NORTHEAST_NE_7' : 'NORTHEAST',
'NORTHEAST_NE_8' : 'NORTHEAST',
'NORTHEAST_NE_9' : 'NORTHEAST',
'SOUTH_CENTRAL_SC_1' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_10' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_2' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_3' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_6' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_7' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_8' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_9' : 'SOUTH CENTRAL',
'SOUTHEAST_SE_1' : 'SOUTHEAST',
'SOUTHEAST_SE_2' : 'SOUTHEAST',
'SOUTHEAST_SE_3' : 'SOUTHEAST',
'SOUTHEAST_SE_4' : 'SOUTHEAST',
'SOUTHEAST_SE_5' : 'SOUTHEAST',
'SOUTHEAST_SE_6' : 'SOUTHEAST',
'SOUTHEAST_SE_7' : 'SOUTHEAST',
'SOUTHEAST_SE_8' : 'SOUTHEAST',
'SOUTHEAST_SE_9' : 'SOUTHEAST',
'WEST_W_1' : 'WEST',
'WEST_W_10' : 'WEST',
'WEST_W_11' : 'WEST',
'WEST_W_2' : 'WEST',
'WEST_W_3' : 'WEST',
'WEST_W_4' : 'WEST',
'WEST_W_5' : 'WEST',
'WEST_W_7' : 'WEST',
'WEST_W_9' : 'WEST'
}

test_df['REGION'] = test_df['REG_TERR'].map(regMap)

calibration_factor = {1:1 ,
                      2: 1,
                      3:1, 
                      4:1,
                      5:1, 
                      6:1 ,
                      7:1 ,
                      8:1 ,
                      9:1 ,
                      10:1 ,
                      11:1 ,
                      12:1}

test_df['calib_fct'] = test_df['month'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']

test_df = test_df.drop(['calib_fct'],axis=1)

ncomPred = test_df.copy()

mdso = data7[data7['SEG_GRP']=='MID_DSO']
mdso['TERR'] = mdso['TERR'].str.replace(' - ','_')
mdso['REGION'] = mdso['REGION'].str.replace(' ','_')

mdso['REG_TERR'] = mdso['REGION']+'_'+mdso['TERR']
mdso['445 Year'] = mdso['445 Year'].astype(int)
mdso['445 Year'] = mdso['445 Year'].astype(str)
mdso['445 Week'] = mdso['445 Week'].astype(str)

mdso = mdso.rename(columns={'Net Amount USD':'sales'})

col = ['445 Year', '445 Week' ,'REG_TERR','PRODUCT']
mdso2 = mdso.groupby(col)['sales'].sum().reset_index()


all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = mdso2['REG_TERR'].unique()
all_products = mdso2['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year',  '445 Week', 'REG_TERR', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, mdso2, on=['445 Year', '445 Week', 'REG_TERR','PRODUCT'], how='left')
merged_df['sales'].fillna(0, inplace=True)

merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR','PRODUCT']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT','REG_TERR']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)
regMap = {'CANADA_CAN_1' : 'CANADA',
'CANADA_CAN_2' : 'CANADA',
'CANADA_CAN_3' : 'CANADA',
'CANADA_CAN_4' : 'CANADA',
'NORTH_CENTRAL_NC_1' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_2' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_3' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_4' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_5' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_6' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_7' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_8' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_9' : 'NORTH CENTRAL',
'NORTHEAST_NE_1' : 'NORTHEAST',
'NORTHEAST_NE_2' : 'NORTHEAST',
'NORTHEAST_NE_3' : 'NORTHEAST',
'NORTHEAST_NE_5' : 'NORTHEAST',
'NORTHEAST_NE_6' : 'NORTHEAST',
'NORTHEAST_NE_7' : 'NORTHEAST',
'NORTHEAST_NE_8' : 'NORTHEAST',
'NORTHEAST_NE_9' : 'NORTHEAST',
'SOUTH_CENTRAL_SC_1' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_10' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_2' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_3' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_6' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_7' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_8' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_9' : 'SOUTH CENTRAL',
'SOUTHEAST_SE_1' : 'SOUTHEAST',
'SOUTHEAST_SE_2' : 'SOUTHEAST',
'SOUTHEAST_SE_3' : 'SOUTHEAST',
'SOUTHEAST_SE_4' : 'SOUTHEAST',
'SOUTHEAST_SE_5' : 'SOUTHEAST',
'SOUTHEAST_SE_6' : 'SOUTHEAST',
'SOUTHEAST_SE_7' : 'SOUTHEAST',
'SOUTHEAST_SE_8' : 'SOUTHEAST',
'SOUTHEAST_SE_9' : 'SOUTHEAST',
'WEST_W_1' : 'WEST',
'WEST_W_10' : 'WEST',
'WEST_W_11' : 'WEST',
'WEST_W_2' : 'WEST',
'WEST_W_3' : 'WEST',
'WEST_W_4' : 'WEST',
'WEST_W_5' : 'WEST',
'WEST_W_7' : 'WEST',
'WEST_W_9' : 'WEST'
}

test_df['REGION'] = test_df['REG_TERR'].map(regMap)

calibration_factor = {1:1 ,
                      2: 1,
                      3:1, 
                      4:1,
                      5:1, 
                      6:1 ,
                      7:1 ,
                      8:1 ,
                      9:1 ,
                      10:1 ,
                      11:1 ,
                      12:1}

test_df['calib_fct'] = test_df['month'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']

test_df = test_df.drop(['calib_fct'],axis=1)

mdsoPred = test_df.copy()
sftw = data7[data7['SEG_GRP']=='SFTW']
sftw = data7[data7['SEG_GRP']=='SFTW']
sftw['TERR'] = sftw['TERR'].str.replace(' - ','_')
sftw['REGION'] = sftw['REGION'].str.replace(' ','_')

sftw['REG_TERR'] = sftw['REGION']+'_'+sftw['TERR']
sftw['445 Year'] = sftw['445 Year'].astype(int)
sftw['445 Year'] = sftw['445 Year'].astype(str)
sftw['445 Week'] = sftw['445 Week'].astype(str)

sftw = sftw.rename(columns={'Net Amount USD':'sales'})

col = ['445 Year', '445 Week' ,'REG_TERR','PRODUCT']
sftw2 = sftw.groupby(col)['sales'].sum().reset_index()


all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = sftw2['REG_TERR'].unique()
all_products = sftw2['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year',  '445 Week', 'REG_TERR', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, sftw2, on=['445 Year', '445 Week', 'REG_TERR','PRODUCT'], how='left')
merged_df['sales'].fillna(0, inplace=True)

merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR','PRODUCT']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT','REG_TERR']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)
regMap = {'CANADA_CAN_1' : 'CANADA',
'CANADA_CAN_2' : 'CANADA',
'CANADA_CAN_3' : 'CANADA',
'CANADA_CAN_4' : 'CANADA',
'NORTH_CENTRAL_NC_1' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_2' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_3' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_4' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_5' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_6' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_7' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_8' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_9' : 'NORTH CENTRAL',
'NORTHEAST_NE_1' : 'NORTHEAST',
'NORTHEAST_NE_2' : 'NORTHEAST',
'NORTHEAST_NE_3' : 'NORTHEAST',
'NORTHEAST_NE_5' : 'NORTHEAST',
'NORTHEAST_NE_6' : 'NORTHEAST',
'NORTHEAST_NE_7' : 'NORTHEAST',
'NORTHEAST_NE_8' : 'NORTHEAST',
'NORTHEAST_NE_9' : 'NORTHEAST',
'SOUTH_CENTRAL_SC_1' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_10' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_2' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_3' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_6' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_7' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_8' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_9' : 'SOUTH CENTRAL',
'SOUTHEAST_SE_1' : 'SOUTHEAST',
'SOUTHEAST_SE_2' : 'SOUTHEAST',
'SOUTHEAST_SE_3' : 'SOUTHEAST',
'SOUTHEAST_SE_4' : 'SOUTHEAST',
'SOUTHEAST_SE_5' : 'SOUTHEAST',
'SOUTHEAST_SE_6' : 'SOUTHEAST',
'SOUTHEAST_SE_7' : 'SOUTHEAST',
'SOUTHEAST_SE_8' : 'SOUTHEAST',
'SOUTHEAST_SE_9' : 'SOUTHEAST',
'WEST_W_1' : 'WEST',
'WEST_W_10' : 'WEST',
'WEST_W_11' : 'WEST',
'WEST_W_2' : 'WEST',
'WEST_W_3' : 'WEST',
'WEST_W_4' : 'WEST',
'WEST_W_5' : 'WEST',
'WEST_W_7' : 'WEST',
'WEST_W_9' : 'WEST'
}

test_df['REGION'] = test_df['REG_TERR'].map(regMap)

calibration_factor = {1:1 ,
                      2: 1,
                      3:1, 
                      4:1,
                      5:1, 
                      6:1 ,
                      7:1 ,
                      8:1 ,
                      9:1 ,
                      10:1 ,
                      11:1 ,
                      12:1}

test_df['calib_fct'] = test_df['month'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']

test_df = test_df.drop(['calib_fct'],axis=1)
sftwPred = test_df.copy()

upgr = data7[data7['SEG_GRP']=='UPGR']
upgr['TERR'] = upgr['TERR'].str.replace(' - ','_')
upgr['REGION'] = upgr['REGION'].str.replace(' ','_')

upgr['REG_TERR'] = upgr['REGION']+'_'+upgr['TERR']
upgr['445 Year'] = upgr['445 Year'].astype(int)
upgr['445 Year'] = upgr['445 Year'].astype(str)
upgr['445 Week'] = upgr['445 Week'].astype(str)

upgr = upgr.rename(columns={'Net Amount USD':'sales'})

col = ['445 Year', '445 Week' ,'REG_TERR','PRODUCT']
upgr2 = upgr.groupby(col)['sales'].sum().reset_index()


all_years = list(range(2019, 2024))
all_weeks = list(range(1, 53)) # assuming 52 weeks in a year
all_territories = upgr2['REG_TERR'].unique()
all_products = upgr2['PRODUCT'].unique()

combinations = list(itertools.product(all_years,all_weeks,all_territories,all_products))
index_df = pd.DataFrame(combinations,columns=['445 Year',  '445 Week', 'REG_TERR', 'PRODUCT'])
index_df['445 Year'] = index_df['445 Year'].astype(str)
index_df['445 Week'] = index_df['445 Week'].astype(str)

# Merge the two dataframes
merged_df = pd.merge(index_df, upgr2, on=['445 Year', '445 Week', 'REG_TERR','PRODUCT'], how='left')
merged_df['sales'].fillna(0, inplace=True)

merged_df['Year_Week'] = merged_df['445 Year']+'_'+merged_df['445 Week'].str.zfill(2)
merged_df['Year_Week_date'] = pd.to_datetime(merged_df['445 Year'] + '-01-01') +  pd.to_timedelta((merged_df['445 Week'].astype(int)-1) * 7, unit='d')
merged_df['Year_Week_Ordinal'] = merged_df['Year_Week_date'].apply(lambda x: x.toordinal())

col = ['445 Year', 'Year_Week_date' ,'REG_TERR','PRODUCT']
merged_df1 = merged_df.groupby(col)['sales'].sum().reset_index()
merged_df1['year'] = merged_df1['Year_Week_date'].dt.year
merged_df1['month'] = merged_df1['Year_Week_date'].dt.month
merged_df1['day'] = merged_df1['Year_Week_date'].dt.day

# Split data into training and validation sets
train_df = merged_df1[merged_df1['445 Year'] < '2023']
test_df = merged_df1[merged_df1['445 Year'] == '2023']
test_df.reset_index(inplace=True)
test_df =test_df.drop(['index'], axis=1)

# One-hot encode categorical fieldsa
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_df[['PRODUCT', 'REG_TERR']])
train_encoded = pd.concat([train_df, pd.DataFrame(encoded_features.toarray())], axis=1)

# Normalize amount
scaler = StandardScaler()
train_encoded['sales'] = scaler.fit_transform(train_df[['sales']])
# train_encoded['pop_change'] = scaler.fit_transform(train_df[['pop_change']])

# Define X and y
X_train = train_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_train = train_encoded['sales']

# 2. Model Training
model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Make sure to preprocess 2023 data similarly before prediction
encoded_2023 = encoder.transform(test_df[['PRODUCT','REG_TERR']])
test_encoded = pd.concat([test_df, pd.DataFrame(encoded_2023.toarray())], axis=1)

# Normalize amount
test_encoded['sales'] = scaler.transform(test_df[['sales']])
# test_encoded['pop_change'] = scaler.transform(test_df[['pop_change']])

# Define X and y
X_test = test_encoded.drop(['445 Year', 'PRODUCT', 'REG_TERR', 'sales','Year_Week_date'], axis=1)
y_test = test_encoded['sales']

predictions = model.predict(X_test)
# Inverse transform to get actual values
test_df['predicted_amount'] = scaler.inverse_transform(predictions.reshape(-1, 1))

# 4. Evaluation (if you have actual values for 2023)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(test_df['sales'], test_df['predicted_amount'], squared=False)

regMap = {'CANADA_CAN_1' : 'CANADA',
'CANADA_CAN_2' : 'CANADA',
'CANADA_CAN_3' : 'CANADA',
'CANADA_CAN_4' : 'CANADA',
'NORTH_CENTRAL_NC_1' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_2' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_3' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_4' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_5' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_6' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_7' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_8' : 'NORTH CENTRAL',
'NORTH_CENTRAL_NC_9' : 'NORTH CENTRAL',
'NORTHEAST_NE_1' : 'NORTHEAST',
'NORTHEAST_NE_2' : 'NORTHEAST',
'NORTHEAST_NE_3' : 'NORTHEAST',
'NORTHEAST_NE_5' : 'NORTHEAST',
'NORTHEAST_NE_6' : 'NORTHEAST',
'NORTHEAST_NE_7' : 'NORTHEAST',
'NORTHEAST_NE_8' : 'NORTHEAST',
'NORTHEAST_NE_9' : 'NORTHEAST',
'SOUTH_CENTRAL_SC_1' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_10' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_2' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_3' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_6' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_7' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_8' : 'SOUTH CENTRAL',
'SOUTH_CENTRAL_SC_9' : 'SOUTH CENTRAL',
'SOUTHEAST_SE_1' : 'SOUTHEAST',
'SOUTHEAST_SE_2' : 'SOUTHEAST',
'SOUTHEAST_SE_3' : 'SOUTHEAST',
'SOUTHEAST_SE_4' : 'SOUTHEAST',
'SOUTHEAST_SE_5' : 'SOUTHEAST',
'SOUTHEAST_SE_6' : 'SOUTHEAST',
'SOUTHEAST_SE_7' : 'SOUTHEAST',
'SOUTHEAST_SE_8' : 'SOUTHEAST',
'SOUTHEAST_SE_9' : 'SOUTHEAST',
'WEST_W_1' : 'WEST',
'WEST_W_10' : 'WEST',
'WEST_W_11' : 'WEST',
'WEST_W_2' : 'WEST',
'WEST_W_3' : 'WEST',
'WEST_W_4' : 'WEST',
'WEST_W_5' : 'WEST',
'WEST_W_7' : 'WEST',
'WEST_W_9' : 'WEST'
}

test_df['REGION'] = test_df['REG_TERR'].map(regMap)

calibration_factor = {1:1 ,
                      2: 1,
                      3:1, 
                      4:1,
                      5:1, 
                      6:1 ,
                      7:1 ,
                      8:1 ,
                      9:1 ,
                      10:1 ,
                      11:1 ,
                      12:1}

test_df['calib_fct'] = test_df['month'].map(calibration_factor)
test_df['predicted_cal'] = test_df['predicted_amount'] * test_df['calib_fct']

test_df = test_df.drop(['calib_fct'],axis=1)
upgrPred = test_df.copy()
instPred = instPred.rename(columns={'REG_TERR_CUST':'REG_TERR'})
tsmPred['SEG_GRP'] = 'TSM'
slaPred['SEG_GRP'] = 'SLA'
stckPred['SEG_GRP'] = 'STCK'
prtsPred['SEG_GRP'] = 'PRTS'
instPred['SEG_GRP'] = 'INST'
edsoPred['SEG_GRP'] = 'EDSO'
ncomPred['SEG_GRP'] = 'NCOM'
mdsoPred['SEG_GRP'] = 'MID_DSO'
sftwPred['SEG_GRP'] = 'SFTW'
upgrPred['SEG_GRP'] = 'UPGR'

tsmPred['DIVISION'] = 'PRODUCT'
slaPred['DIVISION'] = 'SLA'
stckPred['DIVISION'] = 'PRODUCT'
prtsPred['DIVISION'] = 'PARTS'
instPred['DIVISION'] = 'PRODUCT'
edsoPred['DIVISION'] = 'PRODUCT'
ncomPred['DIVISION'] = 'PRODUCT'
mdsoPred['DIVISION'] = 'PRODUCT'
sftwPred['DIVISION'] = 'SOFTWARE'
upgrPred['DIVISION'] = 'UPGRADES'

tsmPred['STOCK_OR_NOT'] = 'USER_ORDER'
slaPred['STOCK_OR_NOT'] = 'USER_ORDER'
stckPred['STOCK_OR_NOT'] = 'STOCK'
prtsPred['STOCK_OR_NOT'] = 'USER_ORDER'
instPred['STOCK_OR_NOT'] = 'USER_ORDER'
edsoPred['STOCK_OR_NOT'] = 'USER_ORDER'
ncomPred['STOCK_OR_NOT'] = 'USER_ORDER'
mdsoPred['STOCK_OR_NOT'] = 'USER_ORDER'
sftwPred['STOCK_OR_NOT'] = 'USER_ORDER'
upgrPred['STOCK_OR_NOT'] = 'USER_ORDER'


tsmPred['COMMISSIONABLE'] = 'COMM'
slaPred['COMMISSIONABLE'] = 'COMM'
stckPred['COMMISSIONABLE'] = 'COMM'
prtsPred['COMMISSIONABLE'] = 'COMM'
instPred['COMMISSIONABLE'] = 'COMM'
edsoPred['COMMISSIONABLE'] = 'COMM'
ncomPred['COMMISSIONABLE'] = 'NON_COMM'
mdsoPred['COMMISSIONABLE'] = 'COMM'
sftwPred['COMMISSIONABLE'] = 'COMM'
upgrPred['COMMISSIONABLE'] = 'COMM'

tsmPred['CUST_SEGMENT'] = 'STANDARD'
slaPred['CUST_SEGMENT'] = 'STANDARD'
stckPred['CUST_SEGMENT'] = 'STANDARD'
prtsPred['CUST_SEGMENT'] = 'STANDARD'
edsoPred['CUST_SEGMENT'] = 'ELITE_DSO'
ncomPred['CUST_SEGMENT'] = 'STANDARD'
mdsoPred['CUST_SEGMENT'] = 'MIDTIER_DSO'
sftwPred['CUST_SEGMENT'] = 'STANDARD'
upgrPred['CUST_SEGMENT'] = 'STANDARD'
fcast = pd.concat([tsmPred,slaPred])
fcast = pd.concat([fcast,stckPred])
fcast = pd.concat([fcast,prtsPred])
fcast = pd.concat([fcast,instPred])
fcast = pd.concat([fcast,edsoPred])
fcast = pd.concat([fcast,ncomPred])
fcast = pd.concat([fcast,mdsoPred])
fcast = pd.concat([fcast,sftwPred])
fcast = pd.concat([fcast,upgrPred])

prc = pd.concat([snsttl[snsttl['445 Year']=='2023'],summary_stats_all2[summary_stats_all2['445 Year']=='2023']])prcGrp = ['2022','2023']
dimPrice = data6[data6['445 Year'].isin(prcGrp)]
dimPrice = dimPrice[dimPrice['DIVISION']=='PRODUCT']
dimPrice = dimPrice[(dimPrice['Net Amount USD']>0) & (dimPrice['Net QTY']>0)]
dimPrice['asp'] = dimPrice['Net Amount USD'] / dimPrice['Net QTY']

priceRange = dimPrice.groupby(['SEG_GRP','PRODUCT']).agg(
    min_price=('asp', 'min'),
    max_price=('asp', 'max'),
    median_price=('asp', 'median'),
    first_quartile=('asp', lambda x: x.quantile(0.25)),
    third_quartile=('asp', lambda x: x.quantile(0.75)),
    mean_price=('asp', 'mean'),
    std_dev=('asp', np.std)
).reset_index()
dimPrice1 = pd.merge(dimPrice,priceRange,on=['SEG_GRP','PRODUCT'],how='left')
dimPrice1["std_dev_away"] = round((dimPrice1["asp"] - dimPrice1["mean_price"]) / dimPrice1["std_dev"],3)
dimPrice1['abs_stdDev_awy'] = dimPrice1['std_dev_away'].abs()

dimPrice2 = dimPrice1[dimPrice1['abs_stdDev_awy']< 1.251]

dimPrice2 =  dimPrice2.drop(['min_price',
       'max_price', 'median_price', 'first_quartile', 'third_quartile',
       'mean_price', 'std_dev', 'std_dev_away', 'abs_stdDev_awy'],axis=1)

priceRange2 = dimPrice2.groupby(['SEG_GRP','PRODUCT']).agg(
    min_price=('asp', 'min'),
    max_price=('asp', 'max'),
    median_price=('asp', 'median'),
    first_quartile=('asp', lambda x: x.quantile(0.25)),
    third_quartile=('asp', lambda x: x.quantile(0.75)),
    mean_price=('asp', 'mean'),
    std_dev=('asp', np.std)
).reset_index()

priceRange2['price'] = np.where((priceRange2['SEG_GRP']+priceRange2['PRODUCT'])=='TSMSENSOR_TITANIUM',priceRange2['mean_price'],
                                (np.where((priceRange2['SEG_GRP']+priceRange2['PRODUCT'])=='TSMSENSOR_IXS',priceRange2['mean_price'],priceRange2['mean_price']+priceRange2['std_dev'])))
priceRange3 = priceRange2.drop(['min_price', 'max_price', 'median_price',
       'first_quartile', 'third_quartile', 'mean_price', 'std_dev',],axis=1)

priceRange3['price'] = priceRange3['price'].round(-1).astype(np.int64)

fcast2 = pd.merge(fcast, priceRange3, on=['SEG_GRP','PRODUCT'],how='left')

data8 = data6.groupby(['445 Year', '445 Quarter',  '445 Month', '445 Week', 'REGION', 'TERR', 'SEG_GRP', 'DIVISION','STOCK_OR_NOT',
       'COMMISSIONABLE', 'PRODUCT', 'CUST_SEGMENT','RESP'],dropna=False).agg({'Net Amount USD':'sum','Net QTY':'sum'}).reset_index()
data8 = data8[data8['445 Year'].isin(['2023','2022'])]

data8 = pd.merge(data8, priceRange3, on=['SEG_GRP','PRODUCT'],how='left')
data8['Net QTY'] = data8['Net Amount USD']/data8['price']

data8['TERR2'] = data8['TERR'].str.replace(' - ','_')
data8['REGION2'] = data8['REGION'].str.replace(' ','_')

data8['REG_TERR'] = data8['REGION2']+'_'+data8['TERR2']
data8 = data8.drop(['TERR2','REGION2'], axis =1)

col = ['445 Year', '445 Quarter', '445 Month', '445 Week', 'REGION', 'TERR',
       'SEG_GRP', 'DIVISION', 'STOCK_OR_NOT', 'COMMISSIONABLE', 'PRODUCT',
       'CUST_SEGMENT', 'RESP', 'Net Amount USD', 'Net QTY', 'REG_TERR']

data8['REG_TERR'] = np.where(data8['SEG_GRP']=='INST', (data8['REGION']+'|'+data8['CUST_SEGMENT']),data8['REG_TERR'])

dimResp = data8[['SEG_GRP','REG_TERR','RESP']]
dimResp = dimResp.drop_duplicates()

fcast3 = pd.merge(fcast2,dimResp,on=['SEG_GRP','REG_TERR'],how='left')
fcast3 = fcast3.rename(columns={'predicted_cal':'Net Amount USD'})

fcast3['date_ref'] = fcast3['Year_Week_date'].astype(str)

week = {
'2023-01-01' : 1,
 '2023-01-08' : 2,
 '2023-01-15' : 3,
 '2023-01-22' : 4,
 '2023-01-29' : 5,
 '2023-02-05' : 6,
 '2023-02-12' : 7,
 '2023-02-19' : 8,
 '2023-02-26' : 9,
 '2023-03-05' : 10,
 '2023-03-12' : 11,
 '2023-03-19' : 12,
 '2023-03-26' : 13,
 '2023-04-02' : 14,
 '2023-04-09' : 15,
 '2023-04-16' : 16,
 '2023-04-23' : 17,
 '2023-04-30' : 18,
 '2023-05-07' : 19,
 '2023-05-14' : 20,
 '2023-05-21' : 21,
 '2023-05-28' : 22,
 '2023-06-04' : 23,
 '2023-06-11' : 24,
 '2023-06-18' : 25,
 '2023-06-25' : 26,
 '2023-07-02' : 27,
 '2023-07-09' : 28,
 '2023-07-16' : 29,
 '2023-07-23' : 30,
 '2023-07-30' : 31,
 '2023-08-06' : 32,
 '2023-08-13' : 33,
 '2023-08-20' : 34,
 '2023-08-27' : 35,
 '2023-09-03' : 36,
 '2023-09-10' : 37,
 '2023-09-17' : 38,
 '2023-09-24' : 39,
 '2023-10-01' : 40,
 '2023-10-08' : 41,
 '2023-10-15' : 42,
 '2023-10-22' : 43,
 '2023-10-29' : 44,
 '2023-11-05' : 45,
 '2023-11-12' : 46,
 '2023-11-19' : 47,
 '2023-11-26' : 48,
 '2023-12-03' : 49,
 '2023-12-10' : 50,
 '2023-12-17' : 51,
 '2023-12-24' : 52
}
    
month = {
 '2023-01-01' : 1,
 '2023-01-08' : 1,
 '2023-01-15' : 1,
 '2023-01-22' : 1,
 '2023-01-29' : 2,
 '2023-02-05' : 2,
 '2023-02-12' : 2,
 '2023-02-19' : 2,
 '2023-02-26' : 3,
 '2023-03-05' : 3,
 '2023-03-12' : 3,
 '2023-03-19' : 3,
 '2023-03-26' : 3,
 '2023-04-02' : 4,
 '2023-04-09' : 4,
 '2023-04-16' : 4,
 '2023-04-23' : 4,
 '2023-04-30' : 5,
 '2023-05-07' : 5,
 '2023-05-14' : 5,
 '2023-05-21' : 5,
 '2023-05-28' : 6,
 '2023-06-04' : 6,
 '2023-06-11' : 6,
 '2023-06-18' : 6,
 '2023-06-25' : 6,
 '2023-07-02' : 7,
 '2023-07-09' : 7,
 '2023-07-16' : 7,
 '2023-07-23' : 7,
 '2023-07-30' : 8,
 '2023-08-06' : 8,
 '2023-08-13' : 8,
 '2023-08-20' : 8,
 '2023-08-27' : 9,
 '2023-09-03' : 9,
 '2023-09-10' : 9,
 '2023-09-17' : 9,
 '2023-09-24' : 9,
 '2023-10-01' : 10,
 '2023-10-08' : 10,
 '2023-10-15' : 10,
 '2023-10-22' : 10,
 '2023-10-29' : 11,
 '2023-11-05' : 11,
 '2023-11-12' : 11,
 '2023-11-19' : 11,
 '2023-11-26' : 12,
 '2023-12-03' : 12,
 '2023-12-10' : 12,
 '2023-12-17' : 12,
 '2023-12-24' : 12
}
    
qtr = {
 '2023-01-01' : 1,
 '2023-01-08' : 1,
 '2023-01-15' : 1,
 '2023-01-22' : 1,
 '2023-01-29' : 1,
 '2023-02-05' : 1,
 '2023-02-12' : 1,
 '2023-02-19' : 1,
 '2023-02-26' : 1,
 '2023-03-05' : 1,
 '2023-03-12' : 1,
 '2023-03-19' : 1,
 '2023-03-26' : 1,
 '2023-04-02' : 2,
 '2023-04-09' : 2,
 '2023-04-16' : 2,
 '2023-04-23' : 2,
 '2023-04-30' : 2,
 '2023-05-07' : 2,
 '2023-05-14' : 2,
 '2023-05-21' : 2,
 '2023-05-28' : 2,
 '2023-06-04' : 2,
 '2023-06-11' : 2,
 '2023-06-18' : 2,
 '2023-06-25' : 2,
 '2023-07-02' : 3,
 '2023-07-09' : 3,
 '2023-07-16' : 3,
 '2023-07-23' : 3,
 '2023-07-30' : 3,
 '2023-08-06' : 3,
 '2023-08-13' : 3,
 '2023-08-20' : 3,
 '2023-08-27' : 3,
 '2023-09-03' : 3,
 '2023-09-10' : 3,
 '2023-09-17' : 3,
 '2023-09-24' : 3,
 '2023-10-01' : 4,
 '2023-10-08' : 4,
 '2023-10-15' : 4,
 '2023-10-22' : 4,
 '2023-10-29' : 4,
 '2023-11-05' : 4,
 '2023-11-12' : 4,
 '2023-11-19' : 4,
 '2023-11-26' : 4,
 '2023-12-03' : 4,
 '2023-12-10' : 4,
 '2023-12-17' : 4,
 '2023-12-24' : 4
}

fcast3['445 Quarter'] = fcast3['date_ref'].map(qtr)
fcast3['445 Month'] = fcast3['date_ref'].map(month)
fcast3['445 Week'] = fcast3['date_ref'].map(week)
fcast3 = fcast3.drop(['date_ref'], axis=1)

fcast3['REGION'] = fcast3['REGION'].str.replace('NORTH_CENTRAL','NORTH CENTRAL')
fcast3['REGION'] = fcast3['REGION'].str.replace('SOUTH_CENTRAL','SOUTH CENTRAL')

col = list(data8.columns)
col.pop(5)
col
plan = fcast3[col]

plan['plan_actual'] = 'PLAN'
col = list(data8.columns)
col.pop(5)
col
actuals = data8[col]
actuals['plan_actual'] = 'ACTUALS'

final = pd.concat([plan,actuals])

final2 = final[final['Net Amount USD']!=0]
final2['DIVISION'] = np.where(final2['DIVISION'].isnull(),'PARTS',final2['DIVISION'])

final2['PRODUCT'].value_counts(dropna=False)

final2['PRODUCT'] = np.where(final2['PRODUCT'].isnull(),'OTHER_OTHER',final2['PRODUCT'])

cat = {
'2D_2D' : '2D',
'2D_CEPH' : '2D',
'2D_CORE' : '2D',
'2D_ESSENTIAL' : '2D',
'2D_PAN' : '2D',
'2D_PREMIER' : '2D',
'2D_UPLIFT' : '2D',
'3D_3D' : '3D',
'3D_CBCT' : '3D',
'3D_CBCT_CEPH' : '3D',
'3D_CORE' : '3D',
'3D_ESSENTIAL' : '3D',
'3D_I_CAT' : '3D',
'3D_OTHER' : '3D',
'3D_PREMIER' : '3D',
'3D_RENEW_DIG' : '3D',
'3D_UPLIFT' : '3D',
'CAMERA_DEXCAM4' : 'OTHER',
'CLINIVIEW_CLINIVIEW' : 'OTHER',
'CR_READER_GXPS_500' : 'OTHER',
'CR_READER_OPTIME' : 'OTHER',
'DEXIS_SOFTWARE_DEXIS_SOFTWARE' : 'SOFTWARE',
'DTX_DTX' : 'SOFTWARE',
'INVIVO_INVIVO' : 'CAMERA',
'NOMAD_ESSENTIAL' : 'NOMAD',
'NOMAD_NOMAD' : 'NOMAD',
'NOMAD_PREMIER' : 'NOMAD',
'NOMAD_PRO_2' : 'NOMAD',
'OTHER_CARIVU' : 'OTHER',
'OTHER_DENOPTIX' : 'OTHER',
'OTHER_DEXCAM' : 'OTHER',
'OTHER_FEES' : 'OTHER',
'OTHER_OTHER' : 'OTHER',
'OTHER_X_GUIDE' : 'OTHER',
'SENSOR_CORE' : 'SENSOR',
'SENSOR_ERGO' : 'SENSOR',
'SENSOR_ESSENTIAL' : 'SENSOR',
'SENSOR_FEES' : 'SENSOR',
'SENSOR_IXS' : 'SENSOR',
'SENSOR_PREMIER' : 'SENSOR',
'SENSOR_SENSOR' : 'SENSOR',
'SENSOR_TITANIUM' : 'SENSOR',
'SOFTWARE_SOFTWARE' : 'SENSOR',
'VIXWIN_VIXWIN' : 'OTHER'                       
}
final2['CATEGORY'] = final2['PRODUCT'].map(cat)
final2['REGION'] = np.where(final2['REGION'].isnull(),'WEST',final2['REGION'])
final2.to_csv('plan_actual.csv')
